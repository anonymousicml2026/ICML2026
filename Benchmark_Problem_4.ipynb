{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "import copy\n",
        "from typing import Dict, Tuple, List\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 0) Matplotlib / Font Settings\n",
        "# ============================================================\n",
        "plt.rcParams.update({\n",
        "    \"pdf.fonttype\": 42,\n",
        "    \"font.family\": \"serif\",\n",
        "    \"font.serif\": [\"Liberation Serif\", \"FreeSerif\", \"serif\"],\n",
        "    \"font.size\": 10,\n",
        "    \"axes.labelsize\": 10,\n",
        "    \"legend.fontsize\": 9,\n",
        "    \"xtick.labelsize\": 10,\n",
        "    \"ytick.labelsize\": 10,\n",
        "    \"mathtext.fontset\": \"stix\",\n",
        "})\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1) Unified Configuration\n",
        "# ============================================================\n",
        "CFG: Dict = {\n",
        "    # reproducibility / device / dtype\n",
        "    \"seed\": 42,\n",
        "    \"device\": \"cuda:2\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"dtype\": torch.float32,\n",
        "\n",
        "    # simulation grid\n",
        "    \"T\": 2,\n",
        "    \"N\": 20,\n",
        "\n",
        "    # delay\n",
        "    \"tau\": 1.0,\n",
        "\n",
        "    # epidemiology params\n",
        "    \"Lambda\": 2.0,\n",
        "    \"beta\": 0.2,\n",
        "    \"alpha\": 0.1,\n",
        "    \"gamma\": 0.2,\n",
        "    \"mu\": 0.1,\n",
        "    \"p\": 0.6,\n",
        "\n",
        "    # cost weights & bounds\n",
        "    \"k1\": 5.0,\n",
        "    \"k2\": 5.0,\n",
        "    \"k3\": 50.0,\n",
        "    \"k4\": 50.0,\n",
        "    \"u_min\": 0.0,\n",
        "    \"u_max\": 1.0,\n",
        "\n",
        "    # initial state\n",
        "    \"S0\": 100.0,\n",
        "    \"I0\": 20.0,\n",
        "    \"C0\": 10.0,\n",
        "    \"R0\": 20.0,\n",
        "\n",
        "    # noise intensity (diag)\n",
        "    \"eta\": [0.1, 0.1, 0.1, 0.1],\n",
        "\n",
        "    # --- FBSM benchmark ---\n",
        "    \"FBSM_TOL\": 1e-7,\n",
        "    \"FBSM_LR\": 0.05,\n",
        "    \"FBSM_MAX_ITER\": 10000,\n",
        "\n",
        "    # --- PG-DPO training ---\n",
        "    \"PG_iter\": 5000,\n",
        "    \"pg_lr\": 1e-4,\n",
        "    \"pg_clip_grad\": 1.0,\n",
        "    \"pg_hidden\": 128,\n",
        "    \"pg_sched_step\": 2000000,\n",
        "    \"pg_sched_gamma\": 0.2,\n",
        "    \"pg_batch_size\": 256,\n",
        "\n",
        "    # --- Stepwise MPC ---\n",
        "    \"mpc_num_mc\": 128,\n",
        "    \"mpc_seed_base\": 42,\n",
        "\n",
        "    # --- FBSDE  ---\n",
        "    \"fbsde_seed\": 42,\n",
        "    \"fbsde_batch\": 256,\n",
        "    \"fbsde_epochs\": 30000,\n",
        "    \"fbsde_lr\": 1e-4,\n",
        "    \"fbsde_step\": 100000,\n",
        "    \"fbsde_hidden\": 96,\n",
        "\n",
        "    # FBSDE loss weights\n",
        "    \"w_L1\": 1.0,\n",
        "    \"w_terminal\": 1.0,\n",
        "    \"w_L2\": 0.4,\n",
        "\n",
        "    # curriculum & EMA target net\n",
        "    \"L2_warmup_epochs\": 800,\n",
        "    \"L2_ramp_epochs\": 1200,\n",
        "    \"ema_decay\": 0.995,\n",
        "\n",
        "    # --- PPO ---\n",
        "    \"ppo_hidden\": 64,\n",
        "    \"ppo_batch\": 256,        # num_envs per update\n",
        "    \"ppo_episodes\": 512000,     # # of total episodes\n",
        "    \"ppo_epochs\": 10,         # PPO epochs per update\n",
        "    \"ppo_clip_eps\": 0.2,\n",
        "    \"ppo_lr\": 5e-5,\n",
        "    \"ppo_gamma_rl\": 1.0,\n",
        "    \"ppo_gae_lambda\": 0.95,\n",
        "    \"ppo_entropy_coef\": 0.01,\n",
        "    \"ppo_max_grad_norm\": 0.5,\n",
        "    \"ppo_minibatch\": 1024,\n",
        "}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2) Global device / dtype / derived params\n",
        "# ============================================================\n",
        "DEVICE = torch.device(CFG[\"device\"])\n",
        "torch.set_default_dtype(CFG[\"dtype\"])\n",
        "\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def compute_derived_cfg(cfg: Dict) -> None:\n",
        "    cfg[\"dt\"] = cfg[\"T\"] / cfg[\"N\"]\n",
        "    cfg[\"K_delay\"] = int(cfg[\"tau\"] / cfg[\"dt\"])  # keep original rule (floor)\n",
        "    cfg[\"N_pop\"] = cfg[\"S0\"] + cfg[\"I0\"] + cfg[\"C0\"] + cfg[\"R0\"]\n",
        "\n",
        "\n",
        "compute_derived_cfg(CFG)\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3) Shared physics helpers (delay / dynamics / cost)\n",
        "# ============================================================\n",
        "def get_delayed_val_np(arr: np.ndarray, t_idx: int, k_delay: int, init_val: float) -> float:\n",
        "    j = t_idx - k_delay\n",
        "    return init_val if j < 0 else arr[j]\n",
        "\n",
        "\n",
        "def to_numpy_noise(noise_tensor, N: int) -> np.ndarray:\n",
        "    if torch.is_tensor(noise_tensor):\n",
        "        noise_np = noise_tensor.detach().cpu().numpy()\n",
        "    else:\n",
        "        noise_np = np.array(noise_tensor)\n",
        "    if noise_np.ndim == 3 and noise_np.shape[1] == 1:\n",
        "        noise_np = noise_np.squeeze(1)\n",
        "    if noise_np.shape[0] != N:\n",
        "        raise ValueError(f\"Noise length {noise_np.shape[0]} does not match N={N}.\")\n",
        "    return noise_np\n",
        "\n",
        "\n",
        "def running_cost_torch(cfg: Dict, r: torch.Tensor, u: torch.Tensor) -> torch.Tensor:\n",
        "    I = r[:, 1]\n",
        "    C = r[:, 2]\n",
        "    return (cfg[\"k1\"] * I + cfg[\"k2\"] * C\n",
        "            + 0.5 * cfg[\"k3\"] * u[:, 0] ** 2\n",
        "            + 0.5 * cfg[\"k4\"] * u[:, 1] ** 2)\n",
        "\n",
        "\n",
        "def step_state_stoch(\n",
        "    cfg: Dict,\n",
        "    r: torch.Tensor,\n",
        "    r_hist_stack: torch.Tensor,\n",
        "    u: torch.Tensor,\n",
        "    w: torch.Tensor,\n",
        "    eta_vec: torch.Tensor\n",
        ") -> torch.Tensor:\n",
        "    S, I, C, R = r.unbind(-1)\n",
        "\n",
        "    hist_len = r_hist_stack.size(0)\n",
        "    idx = max(0, hist_len - 1 - cfg[\"K_delay\"])\n",
        "    I_tau = r_hist_stack[idx, :, 1]\n",
        "    C_tau = r_hist_stack[idx, :, 2]\n",
        "\n",
        "    u1t, u2t = u[:, 0], u[:, 1]\n",
        "    inc = cfg[\"beta\"] * S * (I_tau + C_tau) * (1 - u1t)\n",
        "\n",
        "    dS = cfg[\"Lambda\"] - inc - cfg[\"alpha\"] * S\n",
        "    dI = inc - (cfg[\"alpha\"] + cfg[\"gamma\"] + u2t) * I\n",
        "    dC = (cfg[\"p\"] * cfg[\"gamma\"]) * I - (cfg[\"alpha\"] + cfg[\"mu\"]) * C\n",
        "    dR = ((1 - cfg[\"p\"]) * cfg[\"gamma\"] + u2t) * I - cfg[\"alpha\"] * R\n",
        "\n",
        "    drift = torch.stack([dS, dI, dC, dR], dim=-1)\n",
        "    diffusion = eta_vec[None, :] * r\n",
        "\n",
        "    r_next = r + cfg[\"dt\"] * drift + math.sqrt(cfg[\"dt\"]) * diffusion * w\n",
        "    return torch.clamp(r_next, min=0.0, max=cfg[\"N_pop\"] * 1.5)\n",
        "\n",
        "\n",
        "def cumulative_objective_np(cfg: Dict, I_traj, C_traj, u1_traj, u2_traj) -> np.ndarray:\n",
        "    stage = (cfg[\"k1\"] * I_traj[:-1] + cfg[\"k2\"] * C_traj[:-1]\n",
        "             + 0.5 * cfg[\"k3\"] * (u1_traj ** 2) + 0.5 * cfg[\"k4\"] * (u2_traj ** 2))\n",
        "    return (stage * cfg[\"dt\"]).cumsum()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4) Plotting / Metrics\n",
        "# ============================================================\n",
        "def print_metrics(gt: np.ndarray, preds: Dict[str, np.ndarray], name: str) -> None:\n",
        "    print(f\"\\n[{name}]\")\n",
        "    print(f\"{'Algorithm':<15} | {'RMSE':<12} | {'MAE':<12}\")\n",
        "    print(\"-\" * 45)\n",
        "    for alg, pr in preds.items():\n",
        "        m = min(len(gt), len(pr))\n",
        "        rmse = float(np.sqrt(np.mean((pr[:m] - gt[:m]) ** 2)))\n",
        "        mae = float(np.mean(np.abs(pr[:m] - gt[:m])))\n",
        "        print(f\"{alg:<15} | {rmse:<12.6f} | {mae:<12.6f}\")\n",
        "\n",
        "\n",
        "def plot_series(\n",
        "    t: np.ndarray,\n",
        "    gt: np.ndarray,\n",
        "    series: Dict[str, Tuple[np.ndarray, str]],\n",
        "    title: str,\n",
        "    xlabel: str,\n",
        "    ylabel: str,\n",
        "    filename_pdf: str,\n",
        "    ylim: Tuple[float, float] = None,\n",
        "    ppo: bool = True,\n",
        "    absde: bool = True,\n",
        ") -> None:\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    plt.plot(t, gt, \"k-\", lw=2, label=\"Benchmark\")\n",
        "\n",
        "    for name, (y, style) in series.items():\n",
        "        name_upper = name.upper()\n",
        "        if (\"PPO\" in name_upper) and (not ppo):\n",
        "            continue\n",
        "        if (\"ABSDE\" in name_upper) and (not absde):\n",
        "            continue\n",
        "        plt.plot(t, y, style, label=name)\n",
        "\n",
        "    #plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(ylim)\n",
        "    plt.grid(alpha=0.1)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    stem = filename_pdf[:-4] if filename_pdf.lower().endswith(\".pdf\") else filename_pdf\n",
        "    tag = \"\"\n",
        "    tag += \"_ppo\" if ppo else \"_noPPO\"\n",
        "    tag += \"_absde\" if absde else \"_noABSDE\"\n",
        "    out_pdf = f\"{stem}{tag}.pdf\"\n",
        "\n",
        "    #plt.savefig(out_pdf, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5) Benchmark: FBSM\n",
        "# ============================================================\n",
        "def stochastic_milstein_forward_np(cfg: Dict, u1: np.ndarray, u2: np.ndarray, noise_W: np.ndarray):\n",
        "    N = cfg[\"N\"]\n",
        "    dt = cfg[\"dt\"]\n",
        "    K = cfg[\"K_delay\"]\n",
        "    eta = cfg[\"eta\"]\n",
        "\n",
        "    S = np.zeros(N + 1); I = np.zeros(N + 1); C = np.zeros(N + 1); R = np.zeros(N + 1)\n",
        "    S[0], I[0], C[0], R[0] = cfg[\"S0\"], cfg[\"I0\"], cfg[\"C0\"], cfg[\"R0\"]\n",
        "    sqrt_dt = np.sqrt(dt)\n",
        "\n",
        "    for k in range(N):\n",
        "        I_tau = get_delayed_val_np(I, k, K, cfg[\"I0\"])\n",
        "        C_tau = get_delayed_val_np(C, k, K, cfg[\"C0\"])\n",
        "        xi = noise_W[k]  # (4,)\n",
        "\n",
        "        incidence = cfg[\"beta\"] * S[k] * (I_tau + C_tau) * (1 - u1[k])\n",
        "\n",
        "        drift_S = cfg[\"Lambda\"] - incidence - cfg[\"alpha\"] * S[k]\n",
        "        drift_I = incidence - (cfg[\"alpha\"] + cfg[\"gamma\"] + u2[k]) * I[k]\n",
        "        drift_C = (cfg[\"p\"] * cfg[\"gamma\"]) * I[k] - (cfg[\"alpha\"] + cfg[\"mu\"]) * C[k]\n",
        "        drift_R = ((1 - cfg[\"p\"]) * cfg[\"gamma\"] + u2[k]) * I[k] - cfg[\"alpha\"] * R[k]\n",
        "\n",
        "        diff_S = eta[0] * S[k]\n",
        "        corr_S = 0.5 * (eta[0] ** 2) * S[k] * (xi[0] ** 2 - 1) * dt\n",
        "        S[k + 1] = S[k] + drift_S * dt + diff_S * xi[0] * sqrt_dt + corr_S\n",
        "\n",
        "        diff_I = eta[1] * I[k]\n",
        "        corr_I = 0.5 * (eta[1] ** 2) * I[k] * (xi[1] ** 2 - 1) * dt\n",
        "        I[k + 1] = I[k] + drift_I * dt + diff_I * xi[1] * sqrt_dt + corr_I\n",
        "\n",
        "        diff_C = eta[2] * C[k]\n",
        "        corr_C = 0.5 * (eta[2] ** 2) * C[k] * (xi[2] ** 2 - 1) * dt\n",
        "        C[k + 1] = C[k] + drift_C * dt + diff_C * xi[2] * sqrt_dt + corr_C\n",
        "\n",
        "        diff_R = eta[3] * R[k]\n",
        "        corr_R = 0.5 * (eta[3] ** 2) * R[k] * (xi[3] ** 2 - 1) * dt\n",
        "        R[k + 1] = R[k] + drift_R * dt + diff_R * xi[3] * sqrt_dt + corr_R\n",
        "\n",
        "        S[k + 1] = max(0, S[k + 1]); I[k + 1] = max(0, I[k + 1])\n",
        "        C[k + 1] = max(0, C[k + 1]); R[k + 1] = max(0, R[k + 1])\n",
        "\n",
        "    return S, I, C, R\n",
        "\n",
        "\n",
        "def solve_adjoint_deterministic_np(cfg: Dict, S, I, C, R, u1, u2):\n",
        "    N = cfg[\"N\"]\n",
        "    dt = cfg[\"dt\"]\n",
        "    K = cfg[\"K_delay\"]\n",
        "\n",
        "    c1 = np.zeros(N + 1); c2 = np.zeros(N + 1); c3 = np.zeros(N + 1); c4 = np.zeros(N + 1)\n",
        "    c1[-1] = 0; c2[-1] = 0; c3[-1] = 0; c4[-1] = 0\n",
        "\n",
        "    for k in range(N - 1, -1, -1):\n",
        "        Sk, Ik = S[k], I[k]\n",
        "        Itau = get_delayed_val_np(I, k, K, cfg[\"I0\"])\n",
        "        Ctau = get_delayed_val_np(C, k, K, cfg[\"C0\"])\n",
        "\n",
        "        dc1 = (c1[k + 1] - c2[k + 1]) * (cfg[\"beta\"] * (Itau + Ctau) * (1 - u1[k])) + c1[k + 1] * cfg[\"alpha\"]\n",
        "\n",
        "        dc2 = (-cfg[\"k1\"]\n",
        "               + (c1[k + 1] - c2[k + 1]) * cfg[\"beta\"] * Sk * (1 - u1[k])\n",
        "               + c2[k + 1] * (cfg[\"alpha\"] + cfg[\"gamma\"] + u2[k])\n",
        "               - c3[k + 1] * (cfg[\"p\"] * cfg[\"gamma\"])\n",
        "               - c4[k + 1] * ((1 - cfg[\"p\"]) * cfg[\"gamma\"] + u2[k]))\n",
        "\n",
        "        dc3 = (-cfg[\"k2\"]\n",
        "               + (c1[k + 1] - c2[k + 1]) * cfg[\"beta\"] * Sk * (1 - u1[k])\n",
        "               + c3[k + 1] * (cfg[\"alpha\"] + cfg[\"mu\"]))\n",
        "\n",
        "        dc4 = c4[k + 1] * cfg[\"alpha\"]\n",
        "\n",
        "        c1[k] = c1[k + 1] - dc1 * dt\n",
        "        c2[k] = c2[k + 1] - dc2 * dt\n",
        "        c3[k] = c3[k + 1] - dc3 * dt\n",
        "        c4[k] = c4[k + 1] - dc4 * dt\n",
        "\n",
        "    return c1, c2, c3, c4\n",
        "\n",
        "\n",
        "def solve_fbsm_benchmark(cfg: Dict, noise_input, verbose: bool = True):\n",
        "    noise_W = to_numpy_noise(noise_input, cfg[\"N\"])\n",
        "    N = cfg[\"N\"]\n",
        "\n",
        "    u1 = np.full(N + 1, 0.5)\n",
        "    u2 = np.full(N + 1, 0.5)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[FBSM] Start. Noise shape={noise_W.shape}\")\n",
        "\n",
        "    for it in range(cfg[\"FBSM_MAX_ITER\"]):\n",
        "        S, I, C, R = stochastic_milstein_forward_np(cfg, u1, u2, noise_W)\n",
        "        c1, c2, c3, c4 = solve_adjoint_deterministic_np(cfg, S, I, C, R, u1, u2)\n",
        "\n",
        "        old_u1 = u1.copy()\n",
        "        old_u2 = u2.copy()\n",
        "\n",
        "        for k in range(N + 1):\n",
        "            Itau = get_delayed_val_np(I, k, cfg[\"K_delay\"], cfg[\"I0\"])\n",
        "            Ctau = get_delayed_val_np(C, k, cfg[\"K_delay\"], cfg[\"C0\"])\n",
        "\n",
        "            switching_fn1 = (c2[k] - c1[k]) * S[k] * (Itau + Ctau)\n",
        "            u1_star = np.clip(switching_fn1 / cfg[\"k3\"], cfg[\"u_min\"], cfg[\"u_max\"])\n",
        "\n",
        "            switching_fn2 = (c2[k] - c4[k]) * I[k]\n",
        "            u2_star = np.clip(switching_fn2 / cfg[\"k4\"], cfg[\"u_min\"], cfg[\"u_max\"])\n",
        "\n",
        "            u1[k] = (1 - cfg[\"FBSM_LR\"]) * old_u1[k] + cfg[\"FBSM_LR\"] * u1_star\n",
        "            u2[k] = (1 - cfg[\"FBSM_LR\"]) * old_u2[k] + cfg[\"FBSM_LR\"] * u2_star\n",
        "\n",
        "        diff = np.mean(np.abs(u1 - old_u1)) + np.mean(np.abs(u2 - old_u2))\n",
        "\n",
        "        if diff < cfg[\"FBSM_TOL\"]:\n",
        "            if verbose:\n",
        "                print(f\"[FBSM] Converged at iter={it}, diff={diff:.6e}\")\n",
        "            break\n",
        "\n",
        "        if verbose and it % 100 == 0:\n",
        "            print(f\"[FBSM] Iter={it}, diff={diff:.6e}\")\n",
        "\n",
        "    return u1, u2, S, I, C, R\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 6) PG-DPO policy (LSTM)\n",
        "# ============================================================\n",
        "class LSTMPolicy(nn.Module):\n",
        "    def __init__(self, hidden: int = 64):\n",
        "        super().__init__()\n",
        "        self.hidden = hidden\n",
        "        self.lstm = nn.LSTMCell(4, hidden)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(hidden, 128),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(128, 2),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        nn.init.xavier_uniform_(self.head[2].weight, gain=0.01)\n",
        "        nn.init.constant_(self.head[2].bias, -0.5)\n",
        "\n",
        "    def init_hidden(self, B: int, device: torch.device):\n",
        "        return (torch.zeros(B, self.hidden, device=device),\n",
        "                torch.zeros(B, self.hidden, device=device))\n",
        "\n",
        "    def forward(self, r_t: torch.Tensor, hc, N_pop: float):\n",
        "        r_norm = r_t / N_pop\n",
        "        h, c = self.lstm(r_norm, hc)\n",
        "        u = self.head(h)\n",
        "        return u, (h, c)\n",
        "\n",
        "\n",
        "def train_pg_dpo(cfg: Dict, policy: nn.Module) -> List[float]:\n",
        "    print(\"[PG-DPO] Training...\")\n",
        "    eta_vec = torch.tensor(cfg[\"eta\"], device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "\n",
        "    pg_bs = int(cfg.get(\"pg_batch_size\", 1))\n",
        "\n",
        "    opt = optim.Adam(policy.parameters(), lr=cfg[\"pg_lr\"])\n",
        "    sched = lr_scheduler.StepLR(opt, step_size=cfg[\"pg_sched_step\"], gamma=cfg[\"pg_sched_gamma\"])\n",
        "\n",
        "    loss_hist: List[float] = []\n",
        "    for it in range(cfg[\"PG_iter\"]):\n",
        "        r0 = torch.tensor(\n",
        "            [[cfg[\"S0\"], cfg[\"I0\"], cfg[\"C0\"], cfg[\"R0\"]]],\n",
        "            device=DEVICE, dtype=cfg[\"dtype\"]\n",
        "        ).repeat(pg_bs, 1)\n",
        "\n",
        "        hc = policy.init_hidden(pg_bs, DEVICE)\n",
        "\n",
        "        r_curr = r0\n",
        "        r_hist = [r0]\n",
        "        loss = 0.0\n",
        "\n",
        "        for _ in range(cfg[\"N\"]):\n",
        "            u, hc = policy(r_curr, hc, cfg[\"N_pop\"])\n",
        "\n",
        "            w = torch.randn(pg_bs, 4, device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "\n",
        "            rc = running_cost_torch(cfg, r_curr, u)\n",
        "            if torch.is_tensor(rc) and rc.ndim > 0:\n",
        "                rc = rc.mean()\n",
        "            loss = loss + rc * cfg[\"dt\"]\n",
        "\n",
        "            r_hist_stack = torch.stack(r_hist)  # (t, B, 4)\n",
        "            r_next = step_state_stoch(cfg, r_curr, r_hist_stack, u, w, eta_vec)\n",
        "\n",
        "            r_curr = r_next\n",
        "            r_hist.append(r_next)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(policy.parameters(), cfg[\"pg_clip_grad\"])\n",
        "        opt.step()\n",
        "        sched.step()\n",
        "\n",
        "        loss_hist.append(float(loss.item()))\n",
        "        if it % 500 == 0:\n",
        "            print(f\"[PG-DPO] Iter {it}, Loss {loss.item():.6f}\")\n",
        "\n",
        "    return loss_hist\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 7) Rollouts: (A) Pure policy, (B) Stepwise MPC\n",
        "# ============================================================\n",
        "@torch.no_grad()\n",
        "def rollout_policy_on_noise(cfg: Dict, policy: nn.Module, noise_data: torch.Tensor):\n",
        "    eta_vec = torch.tensor(cfg[\"eta\"], device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "    r_curr = torch.tensor([[cfg[\"S0\"], cfg[\"I0\"], cfg[\"C0\"], cfg[\"R0\"]]], device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "    hc = policy.init_hidden(1, DEVICE)\n",
        "\n",
        "    real_history = [r_curr]\n",
        "    I_traj = [cfg[\"I0\"]]\n",
        "    C_traj = [cfg[\"C0\"]]\n",
        "    u1_traj, u2_traj = [], []\n",
        "\n",
        "    for t in range(cfg[\"N\"]):\n",
        "        u, hc = policy(r_curr, hc, cfg[\"N_pop\"])\n",
        "        w_t = noise_data[t]\n",
        "        r_hist_stack = torch.stack(real_history)\n",
        "        r_next = step_state_stoch(cfg, r_curr, r_hist_stack, u, w_t, eta_vec)\n",
        "\n",
        "        r_curr = r_next\n",
        "        real_history.append(r_curr)\n",
        "\n",
        "        I_traj.append(float(r_curr[0, 1].item()))\n",
        "        C_traj.append(float(r_curr[0, 2].item()))\n",
        "        u1_traj.append(float(u[0, 0].item()))\n",
        "        u2_traj.append(float(u[0, 1].item()))\n",
        "\n",
        "    return np.array(I_traj), np.array(C_traj), np.array(u1_traj), np.array(u2_traj)\n",
        "\n",
        "\n",
        "def run_stepwise_mpc_simulation(cfg: Dict, policy: nn.Module, noise_data: torch.Tensor, num_mc: int, seed_base: int):\n",
        "    eta_vec = torch.tensor(cfg[\"eta\"], device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "    r_curr = torch.tensor([[cfg[\"S0\"], cfg[\"I0\"], cfg[\"C0\"], cfg[\"R0\"]]], device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "    hc_real = policy.init_hidden(1, DEVICE)\n",
        "    real_history = [r_curr]\n",
        "\n",
        "    I_traj = [cfg[\"I0\"]]\n",
        "    C_traj = [cfg[\"C0\"]]\n",
        "    u1_traj, u2_traj = [], []\n",
        "\n",
        "    print(f\"[MPC] Running Stepwise MPC (MC={num_mc})...\")\n",
        "    for t in range(cfg[\"N\"]):\n",
        "        r_now_grad = r_curr.detach().clone().requires_grad_(True)\n",
        "\n",
        "        gen = torch.Generator(device=DEVICE)\n",
        "        gen.manual_seed(seed_base + t)\n",
        "\n",
        "        r_sim = r_now_grad.repeat(num_mc, 1)\n",
        "        hc_sim = (hc_real[0].repeat(num_mc, 1), hc_real[1].repeat(num_mc, 1))\n",
        "\n",
        "        future_cost = 0.0\n",
        "        sim_future_states: List[torch.Tensor] = []\n",
        "\n",
        "        for k in range(t, cfg[\"N\"]):\n",
        "            u_sim, hc_sim = policy(r_sim, hc_sim, cfg[\"N_pop\"])\n",
        "            future_cost = future_cost + running_cost_torch(cfg, r_sim, u_sim).mean() * cfg[\"dt\"]\n",
        "\n",
        "            idx_tau = k - cfg[\"K_delay\"]\n",
        "            if idx_tau < t:\n",
        "                safe_idx = max(0, idx_tau)\n",
        "                r_delayed = real_history[safe_idx].detach().repeat(num_mc, 1)\n",
        "            elif idx_tau == t:\n",
        "                r_delayed = r_now_grad.repeat(num_mc, 1)\n",
        "            else:\n",
        "                future_idx = idx_tau - (t + 1)\n",
        "                r_delayed = sim_future_states[future_idx]\n",
        "\n",
        "            S_s, I_s, C_s, R_s = r_sim.unbind(-1)\n",
        "            I_tau, C_tau = r_delayed[:, 1], r_delayed[:, 2]\n",
        "            u1_s, u2_s = u_sim[:, 0], u_sim[:, 1]\n",
        "\n",
        "            inc = cfg[\"beta\"] * S_s * (I_tau + C_tau) * (1 - u1_s)\n",
        "\n",
        "            dS = cfg[\"Lambda\"] - inc - cfg[\"alpha\"] * S_s\n",
        "            dI = inc - (cfg[\"alpha\"] + cfg[\"gamma\"] + u2_s) * I_s\n",
        "            dC = (cfg[\"p\"] * cfg[\"gamma\"]) * I_s - (cfg[\"alpha\"] + cfg[\"mu\"]) * C_s\n",
        "            dR = ((1 - cfg[\"p\"]) * cfg[\"gamma\"] + u2_s) * I_s - cfg[\"alpha\"] * R_s\n",
        "\n",
        "            drift = torch.stack([dS, dI, dC, dR], dim=-1)\n",
        "\n",
        "            xi = torch.randn(num_mc, 4, device=DEVICE, generator=gen, dtype=cfg[\"dtype\"])\n",
        "            diffusion = (eta_vec[None, :] * r_sim) * math.sqrt(cfg[\"dt\"]) * xi\n",
        "\n",
        "            r_next_sim = torch.clamp(r_sim + cfg[\"dt\"] * drift + diffusion, 0.0, cfg[\"N_pop\"] * 1.5)\n",
        "            sim_future_states.append(r_next_sim)\n",
        "            r_sim = r_next_sim\n",
        "\n",
        "        grads = torch.autograd.grad(future_cost, r_now_grad, create_graph=False)[0]\n",
        "        lamS, lamI, lamC, lamR = grads[0]\n",
        "\n",
        "        idx_tau_curr = max(0, t - cfg[\"K_delay\"])\n",
        "        r_tau_curr = real_history[idx_tau_curr]\n",
        "        I_tau_curr = r_tau_curr[0, 1]\n",
        "        C_tau_curr = r_tau_curr[0, 2]\n",
        "\n",
        "        S_curr_val = r_curr[0, 0]\n",
        "        I_curr_val = r_curr[0, 1]\n",
        "\n",
        "        val1 = (lamI - lamS) * cfg[\"beta\"] * S_curr_val * (I_tau_curr + C_tau_curr) / cfg[\"k3\"]\n",
        "        val2 = (lamI - lamR) * I_curr_val / cfg[\"k4\"]\n",
        "\n",
        "        u1_star = torch.clamp(val1, cfg[\"u_min\"], cfg[\"u_max\"])\n",
        "        u2_star = torch.clamp(val2, cfg[\"u_min\"], cfg[\"u_max\"])\n",
        "        u_star = torch.stack([u1_star, u2_star], dim=0).unsqueeze(0)\n",
        "\n",
        "        w_t = noise_data[t]\n",
        "        r_hist_stack = torch.stack(real_history)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, hc_real = policy(r_curr, hc_real, cfg[\"N_pop\"])\n",
        "\n",
        "        r_next_real = step_state_stoch(cfg, r_curr, r_hist_stack, u_star, w_t, eta_vec)\n",
        "\n",
        "        r_curr = r_next_real\n",
        "        real_history.append(r_curr)\n",
        "\n",
        "        I_traj.append(float(r_curr[0, 1].item()))\n",
        "        C_traj.append(float(r_curr[0, 2].item()))\n",
        "        u1_traj.append(float(u1_star.item()))\n",
        "        u2_traj.append(float(u2_star.item()))\n",
        "\n",
        "        if t % 10 == 0:\n",
        "            print(f\"[MPC] Step {t}/{cfg['N']} done.\")\n",
        "\n",
        "    return np.array(I_traj), np.array(C_traj), np.array(u1_traj), np.array(u2_traj)\n",
        "\n",
        "\n",
        "#============================================================\n",
        "# 8) Deep ABSDE (FBSDE-Net)\n",
        "#   - Net outputs (Y, Z, G)\n",
        "#       G(t_i) ≈ E_t[ ∂_{x_delay} H(t_i + tau) ]  (here delay is (I_tau, C_tau) -> dim=2)\n",
        "#   - L1: BSDE consistency using tildeY\n",
        "#   - L2: shift-matching   G_i ≈ (∂_{x_delay}H)_{i+D}\n",
        "#============================================================\n",
        "STATE_DIM = 4\n",
        "BROWN_DIM = 4\n",
        "\n",
        "class DeepABSDE_Net(nn.Module):\n",
        "    def __init__(self, hidden: int):\n",
        "        super().__init__()\n",
        "        self.hidden = hidden\n",
        "        self.lstm = nn.LSTMCell(input_size=STATE_DIM + 1, hidden_size=hidden)\n",
        "\n",
        "        self.head_Y = nn.Sequential(\n",
        "            nn.Linear(hidden, hidden), nn.Tanh(), nn.Linear(hidden, STATE_DIM)\n",
        "        )\n",
        "        self.head_Z = nn.Sequential(\n",
        "            nn.Linear(hidden, hidden), nn.Tanh(), nn.Linear(hidden, STATE_DIM * BROWN_DIM)\n",
        "        )\n",
        "        self.head_G = nn.Sequential(\n",
        "            nn.Linear(hidden, hidden), nn.Tanh(), nn.Linear(hidden, 2)\n",
        "        )\n",
        "\n",
        "    def init_hidden(self, batch: int, device: torch.device):\n",
        "        return (\n",
        "            torch.zeros(batch, self.hidden, device=device),\n",
        "            torch.zeros(batch, self.hidden, device=device),\n",
        "        )\n",
        "\n",
        "    def step(self, x, t, hc):\n",
        "        inp = torch.cat([x, t], dim=1)\n",
        "        h, c = self.lstm(inp, hc)\n",
        "\n",
        "        y = self.head_Y(h)\n",
        "        z = self.head_Z(h).view(-1, STATE_DIM, BROWN_DIM)\n",
        "        g = self.head_G(h)\n",
        "\n",
        "        return y, z, g, (h, c)\n",
        "\n",
        "def build_fbsde_helpers(cfg: Dict):\n",
        "    D = int(cfg[\"K_delay\"])\n",
        "    dL_dx = torch.tensor(\n",
        "        [0.0, cfg[\"k1\"], cfg[\"k2\"], 0.0],\n",
        "        device=DEVICE, dtype=cfg[\"dtype\"]\n",
        "    ).view(1, 4)\n",
        "    eta_vec = torch.tensor(\n",
        "        cfg[\"eta\"], device=DEVICE, dtype=cfg[\"dtype\"]\n",
        "    ).view(1, 4)\n",
        "    return D, dL_dx, eta_vec\n",
        "\n",
        "\n",
        "def fbsde_drift(cfg: Dict, x, I_delay, C_delay, u1, u2):\n",
        "    S, I, C, R = x[:, 0:1], x[:, 1:2], x[:, 2:3], x[:, 3:4]\n",
        "    Qd = I_delay + C_delay\n",
        "\n",
        "    bS = cfg[\"Lambda\"] - cfg[\"beta\"] * S * Qd * (1.0 - u1) - cfg[\"alpha\"] * S\n",
        "    bI = cfg[\"beta\"] * S * Qd * (1.0 - u1) - (cfg[\"alpha\"] + cfg[\"gamma\"] + u2) * I\n",
        "    bC = (cfg[\"p\"] * cfg[\"gamma\"]) * I - (cfg[\"alpha\"] + cfg[\"mu\"]) * C  # u2 removed\n",
        "    bR = ((1.0 - cfg[\"p\"]) * cfg[\"gamma\"] + u2) * I - cfg[\"alpha\"] * R\n",
        "\n",
        "    return torch.cat([bS, bI, bC, bR], dim=1)\n",
        "\n",
        "\n",
        "def fbsde_diffusion(eta_vec, x):\n",
        "    return eta_vec * x\n",
        "\n",
        "\n",
        "def fbsde_controls(cfg: Dict, x, I_delay, C_delay, y):\n",
        "    S, I = x[:, 0:1], x[:, 1:2]\n",
        "    Qd = I_delay + C_delay\n",
        "    yS, yI, yR = y[:, 0:1], y[:, 1:2], y[:, 3:4]\n",
        "\n",
        "    u1_raw = (cfg[\"beta\"] * S * Qd * (yI - yS)) / (cfg[\"k3\"] + 1e-12)\n",
        "    u2_raw = (I * (yI - yR)) / (cfg[\"k4\"] + 1e-12)\n",
        "\n",
        "    return (\n",
        "        torch.clamp(u1_raw, cfg[\"u_min\"], cfg[\"u_max\"]),\n",
        "        torch.clamp(u2_raw, cfg[\"u_min\"], cfg[\"u_max\"]),\n",
        "    )\n",
        "\n",
        "\n",
        "def dH_d_delay_target(cfg: Dict, X, U, Y):\n",
        "    S = X[:, 0:1]\n",
        "    u1 = U[:, 0:1]\n",
        "    yS = Y[:, 0:1]\n",
        "    yI = Y[:, 1:2]\n",
        "\n",
        "    g = cfg[\"beta\"] * S * (1.0 - u1) * (yI - yS)\n",
        "    return torch.cat([g, g], dim=1)  # (B,2)\n",
        "\n",
        "\n",
        "def fbsde_driver(cfg: Dict, dL_dx, eta_vec, x, I_delay, C_delay, u1, u2, y, z, g):\n",
        "    yS, yI, yC, yR = y[:, 0:1], y[:, 1:2], y[:, 2:3], y[:, 3:4]\n",
        "    Qd = I_delay + C_delay\n",
        "\n",
        "    dbS_dS = -cfg[\"beta\"] * Qd * (1.0 - u1) - cfg[\"alpha\"]\n",
        "    dbI_dS =  cfg[\"beta\"] * Qd * (1.0 - u1)\n",
        "    At_y_S = yS * dbS_dS + yI * dbI_dS\n",
        "\n",
        "    dbI_dI = -(cfg[\"alpha\"] + cfg[\"gamma\"] + u2)\n",
        "    dbC_dI = (cfg[\"p\"] * cfg[\"gamma\"])\n",
        "    dbR_dI = ((1.0 - cfg[\"p\"]) * cfg[\"gamma\"] + u2)\n",
        "    At_y_I = yI * dbI_dI + yC * dbC_dI + yR * dbR_dI\n",
        "\n",
        "    At_y_C = yC * (-(cfg[\"alpha\"] + cfg[\"mu\"]))\n",
        "    At_y_R = yR * (-cfg[\"alpha\"])\n",
        "\n",
        "    At_y = torch.cat([At_y_S, At_y_I, At_y_C, At_y_R], dim=1)\n",
        "\n",
        "    sigma_term = torch.stack([\n",
        "        eta_vec[0, 0] * z[:, 0, 0],\n",
        "        eta_vec[0, 1] * z[:, 1, 1],\n",
        "        eta_vec[0, 2] * z[:, 2, 2],\n",
        "        eta_vec[0, 3] * z[:, 3, 3],\n",
        "    ], dim=1)\n",
        "\n",
        "    gI, gC = g[:, 0:1], g[:, 1:2]\n",
        "    ant = torch.cat([torch.zeros_like(gI), gI, gC, torch.zeros_like(gI)], dim=1)\n",
        "\n",
        "    return dL_dx + At_y + sigma_term + ant\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Training step\n",
        "# ------------------------------------------------------------\n",
        "def fbsde_train_step(cfg: Dict, net, opt, scheduler, epoch: int):\n",
        "    net.train()\n",
        "    opt.zero_grad()\n",
        "\n",
        "    D, dL_dx, eta_vec = build_fbsde_helpers(cfg)\n",
        "    M, N, dt = cfg[\"fbsde_batch\"], cfg[\"N\"], cfg[\"dt\"]\n",
        "\n",
        "    dW = torch.randn(M, N, 4, device=DEVICE, dtype=cfg[\"dtype\"]) * math.sqrt(dt)\n",
        "\n",
        "    x0 = torch.tensor(\n",
        "        [cfg[\"S0\"], cfg[\"I0\"], cfg[\"C0\"], cfg[\"R0\"]],\n",
        "        device=DEVICE, dtype=cfg[\"dtype\"]\n",
        "    ).view(1, 4).repeat(M, 1)\n",
        "\n",
        "    hc = net.init_hidden(M, DEVICE)\n",
        "    hist0 = x0\n",
        "\n",
        "    x_list = [x0]\n",
        "    y_list, z_list, g_list, u_list = [], [], [], []\n",
        "\n",
        "    for i in range(N):\n",
        "        t_i = torch.full((M, 1), i * dt, device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "        x_i = x_list[i]\n",
        "\n",
        "        y_i, z_i, g_i, hc = net.step(x_i, t_i, hc)\n",
        "        if i > N - D:\n",
        "            g_i = torch.zeros_like(g_i)\n",
        "\n",
        "        x_delay = x_list[i - D] if i - D >= 0 else hist0\n",
        "        I_d, C_d = x_delay[:, 1:2], x_delay[:, 2:3]\n",
        "\n",
        "        u1_i, u2_i = fbsde_controls(cfg, x_i, I_d, C_d, y_i)\n",
        "        b_i = fbsde_drift(cfg, x_i, I_d, C_d, u1_i, u2_i)\n",
        "        sig_i = fbsde_diffusion(eta_vec, x_i)\n",
        "\n",
        "        x_next = torch.clamp(x_i + b_i * dt + sig_i * dW[:, i, :], min=0.0)\n",
        "        x_list.append(x_next)\n",
        "\n",
        "        y_list.append(y_i)\n",
        "        z_list.append(z_i)\n",
        "        g_list.append(g_i)\n",
        "        u_list.append(torch.cat([u1_i, u2_i], dim=1))\n",
        "\n",
        "    t_N = torch.full((M, 1), N * dt, device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "    y_N, _, _, _ = net.step(x_list[N], t_N, hc)\n",
        "    y_list.append(y_N)\n",
        "\n",
        "    X = torch.stack(x_list, dim=1)\n",
        "    Y = torch.stack(y_list, dim=1)\n",
        "    Z = torch.stack(z_list, dim=1)\n",
        "    G = torch.stack(g_list, dim=1)\n",
        "    U = torch.stack(u_list, dim=1)\n",
        "\n",
        "    f_list = []\n",
        "    for i in range(N):\n",
        "        x_delay = X[:, i - D, :] if i - D >= 0 else hist0\n",
        "        f_i = fbsde_driver(\n",
        "            cfg, dL_dx, eta_vec,\n",
        "            X[:, i, :],\n",
        "            x_delay[:, 1:2], x_delay[:, 2:3],\n",
        "            U[:, i, 0:1], U[:, i, 1:2],\n",
        "            Y[:, i, :], Z[:, i, :, :],\n",
        "            G[:, i, :]\n",
        "        )\n",
        "        f_list.append(f_i)\n",
        "    F = torch.stack(f_list, dim=1)\n",
        "\n",
        "    ZdW = torch.einsum(\"bnij,bnj->bni\", Z, dW)\n",
        "    tildeY = torch.zeros_like(Y)\n",
        "    tildeY[:, 0, :] = Y[:, 0, :]\n",
        "    tildeY[:, 1:, :] = Y[:, :-1, :] - F * dt + ZdW\n",
        "\n",
        "    L1 = torch.mean((Y[:, 1:, :] - tildeY[:, 1:, :]) ** 2)\n",
        "    Lterm = torch.mean(Y[:, -1, :] ** 2)\n",
        "\n",
        "    g_targets = []\n",
        "    for k in range(N):\n",
        "        gk = dH_d_delay_target(\n",
        "            cfg,\n",
        "            X[:, k, :].detach(),\n",
        "            U[:, k, :].detach(),\n",
        "            Y[:, k, :].detach(),\n",
        "        )\n",
        "        g_targets.append(gk)\n",
        "\n",
        "    g_targets.append(\n",
        "        torch.zeros(M, 2, device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "    )\n",
        "    G_targ = torch.stack(g_targets, dim=1)\n",
        "\n",
        "    if D < N:\n",
        "        pred = G[:, 0:(N - D + 1), :]\n",
        "        targ = G_targ[:, D:(N + 1), :].detach()\n",
        "        L2 = torch.mean((pred - targ) ** 2)\n",
        "    else:\n",
        "        L2 = torch.tensor(0.0, device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "\n",
        "    wL2 = cfg[\"w_L2\"]\n",
        "    loss = cfg[\"w_L1\"] * L1 + cfg[\"w_terminal\"] * Lterm + wL2 * L2\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
        "    opt.step()\n",
        "    if scheduler is not None:\n",
        "        scheduler.step()\n",
        "\n",
        "    return float(loss), float(L1), float(Lterm), float(L2), float(wL2)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Rollout\n",
        "# ------------------------------------------------------------\n",
        "@torch.no_grad()\n",
        "def rollout_fbsde_on_noise(cfg: Dict, net, noise_data):\n",
        "    net.eval()\n",
        "    D, _, eta_vec = build_fbsde_helpers(cfg)\n",
        "\n",
        "    r_curr = torch.tensor(\n",
        "        [[cfg[\"S0\"], cfg[\"I0\"], cfg[\"C0\"], cfg[\"R0\"]]],\n",
        "        device=DEVICE, dtype=cfg[\"dtype\"]\n",
        "    )\n",
        "    hc = net.init_hidden(1, DEVICE)\n",
        "    hist0 = r_curr.clone()\n",
        "    real_history = [r_curr]\n",
        "\n",
        "    I_traj = [cfg[\"I0\"]]\n",
        "    C_traj = [cfg[\"C0\"]]\n",
        "    u1_traj, u2_traj = [], []\n",
        "\n",
        "    for t in range(cfg[\"N\"]):\n",
        "        t_tensor = torch.full((1, 1), t * cfg[\"dt\"], device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "        y, _, _, hc = net.step(r_curr, t_tensor, hc)\n",
        "\n",
        "        idx = t - D\n",
        "        r_delay = real_history[idx] if idx >= 0 else hist0\n",
        "        I_d, C_d = r_delay[:, 1:2], r_delay[:, 2:3]\n",
        "\n",
        "        u1, u2 = fbsde_controls(cfg, r_curr, I_d, C_d, y)\n",
        "\n",
        "        w = noise_data[t]\n",
        "        b = fbsde_drift(cfg, r_curr, I_d, C_d, u1, u2)\n",
        "        sig = fbsde_diffusion(eta_vec, r_curr)\n",
        "\n",
        "        r_curr = torch.clamp(\n",
        "            r_curr + b * cfg[\"dt\"] + sig * math.sqrt(cfg[\"dt\"]) * w,\n",
        "            min=0.0\n",
        "        )\n",
        "        real_history.append(r_curr)\n",
        "\n",
        "        I_traj.append(float(r_curr[0, 1]))\n",
        "        C_traj.append(float(r_curr[0, 2]))\n",
        "        u1_traj.append(float(u1))\n",
        "        u2_traj.append(float(u2))\n",
        "\n",
        "    return np.array(I_traj), np.array(C_traj), np.array(u1_traj), np.array(u2_traj)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Entry for pipeline\n",
        "# ------------------------------------------------------------\n",
        "def train_fbsde(cfg: Dict):\n",
        "    print(\"\\n[ABSDE] Training Started...\")\n",
        "    torch.manual_seed(cfg[\"fbsde_seed\"])\n",
        "\n",
        "    net = DeepABSDE_Net(cfg[\"fbsde_hidden\"]).to(DEVICE)\n",
        "    opt = optim.Adam(net.parameters(), lr=cfg[\"fbsde_lr\"])\n",
        "    scheduler = optim.lr_scheduler.StepLR(opt, step_size=cfg[\"fbsde_step\"], gamma=0.5)\n",
        "\n",
        "    for ep in range(1, cfg[\"fbsde_epochs\"] + 1):\n",
        "        loss, L1, LT, L2, wL2 = fbsde_train_step(cfg, net, opt, scheduler, ep)\n",
        "        if ep % 3000 == 0:\n",
        "            print(f\"[ABSDE] Ep {ep}, Loss={loss:.4e}, L1={L1:.3e}, LT={LT:.3e}, L2={L2:.3e}\")\n",
        "\n",
        "    return net\n",
        "\n",
        "@torch.no_grad()\n",
        "def rollout_fbsde_on_noise(cfg: Dict, net, noise_data):\n",
        "    net.eval()\n",
        "    D, _, eta_vec = build_fbsde_helpers(cfg)\n",
        "\n",
        "    r_curr = torch.tensor([[cfg[\"S0\"], cfg[\"I0\"], cfg[\"C0\"], cfg[\"R0\"]]], device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "    hc = net.init_hidden(1, DEVICE)\n",
        "    hist0 = r_curr.clone()\n",
        "    real_history = [r_curr]\n",
        "\n",
        "    I_traj = [cfg[\"I0\"]]; C_traj = [cfg[\"C0\"]]\n",
        "    u1_traj, u2_traj = [], []\n",
        "\n",
        "    for t in range(cfg[\"N\"]):\n",
        "        t_tensor = torch.full((1, 1), float(t) * cfg[\"dt\"], device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "        y_curr, _, _, hc = net.step(r_curr, t_tensor, hc)\n",
        "\n",
        "        idx_delay = t - D\n",
        "        r_delay = real_history[idx_delay] if idx_delay >= 0 else hist0\n",
        "        I_d, C_d = r_delay[:, 1:2], r_delay[:, 2:3]\n",
        "\n",
        "        u1, u2 = fbsde_controls(cfg, r_curr, I_d, C_d, y_curr)\n",
        "\n",
        "        w_t = noise_data[t]  # (1,4)\n",
        "        b = fbsde_drift(cfg, r_curr, I_d, C_d, u1, u2)\n",
        "        sig = fbsde_diffusion(eta_vec, r_curr)\n",
        "\n",
        "        r_next = torch.clamp(r_curr + b * cfg[\"dt\"] + sig * w_t, min=0.0)\n",
        "        real_history.append(r_next)\n",
        "        r_curr = r_next\n",
        "\n",
        "        I_traj.append(float(r_curr[0, 1].item()))\n",
        "        C_traj.append(float(r_curr[0, 2].item()))\n",
        "        u1_traj.append(float(u1.item()))\n",
        "        u2_traj.append(float(u2.item()))\n",
        "\n",
        "    return np.array(I_traj), np.array(C_traj), np.array(u1_traj), np.array(u2_traj)\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 9) PPO\n",
        "# ============================================================\n",
        "class WindowHCVPPO(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.actor_mu = nn.Linear(hidden_dim, 2)\n",
        "        self.actor_log_std = nn.Parameter(torch.zeros(2) - 0.5)\n",
        "        self.critic = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        h = out[:, -1]\n",
        "        mu = self.actor_mu(h)\n",
        "        std = torch.exp(self.actor_log_std).expand_as(mu)\n",
        "        v = self.critic(h)\n",
        "        return mu, std, v\n",
        "\n",
        "    def act(self, x, deterministic: bool = False):\n",
        "        mu, std, v = self.forward(x)\n",
        "        if deterministic:\n",
        "            u = torch.sigmoid(mu)\n",
        "            return u, v\n",
        "        dist = torch.distributions.Normal(mu, std)\n",
        "        raw = dist.sample()\n",
        "        u = torch.sigmoid(raw)\n",
        "        logp = dist.log_prob(raw).sum(dim=1, keepdim=True)\n",
        "        return u, logp, v, raw\n",
        "\n",
        "\n",
        "def ppo_train(cfg):\n",
        "    print(\"[PPO] Starting Training...\")\n",
        "\n",
        "    dt_local = cfg[\"dt\"]\n",
        "    delay_steps = int(round(cfg[\"tau\"] / dt_local))\n",
        "    window = delay_steps + 2\n",
        "    input_dim = 7\n",
        "\n",
        "    total_episodes = int(cfg[\"ppo_episodes\"])\n",
        "    # number of parallel episodes per rollout/update\n",
        "    num_envs = int(cfg[\"ppo_batch\"])\n",
        "\n",
        "    agent = WindowHCVPPO(input_dim, cfg[\"ppo_hidden\"]).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        agent.actor_mu.bias.data[:] = 1.0\n",
        "    opt = optim.Adam(agent.parameters(), lr=cfg[\"ppo_lr\"])\n",
        "\n",
        "    episodes_done = 0\n",
        "    update_idx = 0\n",
        "\n",
        "    while episodes_done < total_episodes:\n",
        "        update_idx += 1\n",
        "\n",
        "        batch_envs = min(num_envs, total_episodes - episodes_done)\n",
        "        episodes_done += batch_envs\n",
        "\n",
        "        S = torch.full((batch_envs, 1), cfg[\"S0\"], device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "        I = torch.full((batch_envs, 1), cfg[\"I0\"], device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "        C = torch.full((batch_envs, 1), cfg[\"C0\"], device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "        R = torch.full((batch_envs, 1), cfg[\"R0\"], device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "\n",
        "        hist_I = [I.clone() for _ in range(delay_steps + 1)]\n",
        "        hist_C = [C.clone() for _ in range(delay_steps + 1)]\n",
        "        win = torch.zeros(batch_envs, window, input_dim, device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "\n",
        "        buf_W, buf_raw, buf_lp, buf_v, buf_r = [], [], [], [], []\n",
        "        ep_reward = 0.0\n",
        "\n",
        "        agent.eval()\n",
        "        with torch.no_grad():\n",
        "            for n in range(cfg[\"N\"]):\n",
        "                u, logp, v, raw = agent.act(win, deterministic=False)\n",
        "                u1, u2 = u[:, 0:1], u[:, 1:2]\n",
        "\n",
        "                I_tau = hist_I[-(delay_steps + 1)]\n",
        "                C_tau = hist_C[-(delay_steps + 1)]\n",
        "\n",
        "                dW = torch.randn(batch_envs, 4, device=DEVICE, dtype=cfg[\"dtype\"]) * np.sqrt(dt_local)\n",
        "\n",
        "                S = S + (cfg[\"Lambda\"] - cfg[\"beta\"] * S * (I_tau + C_tau) * (1 - u1) - cfg[\"alpha\"] * S) * dt_local \\\n",
        "                      + cfg[\"eta\"][0] * S * dW[:, 0:1]\n",
        "\n",
        "                I = I + (cfg[\"beta\"] * S * (I_tau + C_tau) * (1 - u1) - (cfg[\"alpha\"] + cfg[\"gamma\"] + u2) * I) * dt_local \\\n",
        "                      + cfg[\"eta\"][1] * I * dW[:, 1:2]\n",
        "\n",
        "                C = C + ((cfg[\"p\"] * cfg[\"gamma\"]) * I - (cfg[\"alpha\"] + cfg[\"mu\"]) * C) * dt_local \\\n",
        "                      + cfg[\"eta\"][2] * C * dW[:, 2:3]\n",
        "\n",
        "                R = R + (((1 - cfg[\"p\"]) * cfg[\"gamma\"] + u2) * I - cfg[\"alpha\"] * R) * dt_local \\\n",
        "                      + cfg[\"eta\"][3] * R * dW[:, 3:4]\n",
        "\n",
        "                hist_I.append(I); hist_C.append(C)\n",
        "\n",
        "                cost = (cfg[\"k1\"] * I + cfg[\"k2\"] * C + 0.5 * cfg[\"k3\"] * u1 ** 2 + 0.5 * cfg[\"k4\"] * u2 ** 2).squeeze()\n",
        "                reward = -cost * dt_local\n",
        "\n",
        "                buf_W.append(win.clone())\n",
        "                buf_raw.append(raw.clone())\n",
        "                buf_lp.append(logp.clone())\n",
        "                buf_v.append(v.clone())\n",
        "                buf_r.append(reward.clone())\n",
        "\n",
        "                ep_reward += reward.mean().item()\n",
        "\n",
        "                if n < cfg[\"N\"] - 1:\n",
        "                    new = torch.cat([\n",
        "                        torch.full((batch_envs, 1, 1), (n + 1) * dt_local, device=DEVICE, dtype=cfg[\"dtype\"]),\n",
        "                        S.unsqueeze(1), I.unsqueeze(1), C.unsqueeze(1), R.unsqueeze(1),\n",
        "                        I_tau.unsqueeze(1), C_tau.unsqueeze(1)\n",
        "                    ], dim=2)\n",
        "                    win = torch.cat([win[:, 1:], new], dim=1)\n",
        "\n",
        "        # -----------------------------\n",
        "        # GAE (terminal bootstrap = 0)\n",
        "        # -----------------------------\n",
        "        returns = []\n",
        "        gae = torch.zeros(batch_envs, device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "\n",
        "        for t_ in reversed(range(cfg[\"N\"])):\n",
        "            v_t = buf_v[t_].squeeze(-1)\n",
        "            v_next = buf_v[t_ + 1].squeeze(-1) if t_ < cfg[\"N\"] - 1 else torch.zeros_like(v_t)\n",
        "            delta = buf_r[t_] + cfg[\"ppo_gamma_rl\"] * v_next - v_t\n",
        "            gae = delta + cfg[\"ppo_gamma_rl\"] * cfg[\"ppo_gae_lambda\"] * gae\n",
        "            returns.insert(0, (gae + v_t).unsqueeze(-1))\n",
        "\n",
        "        # -----------------------------\n",
        "        # Flatten buffers\n",
        "        # -----------------------------\n",
        "        W = torch.stack(buf_W).reshape(-1, window, input_dim).detach()\n",
        "        RAW = torch.stack(buf_raw).reshape(-1, 2).detach()\n",
        "        OLD_LP = torch.stack(buf_lp).reshape(-1, 1).detach()\n",
        "        RET = torch.stack(returns).reshape(-1, 1).detach()\n",
        "        OLD_V = torch.stack(buf_v).reshape(-1, 1).detach()\n",
        "\n",
        "        ADV = (RET - OLD_V)\n",
        "        ADV = (ADV - ADV.mean()) / (ADV.std() + 1e-8)\n",
        "\n",
        "        # -----------------------------\n",
        "        # PPO Update\n",
        "        # -----------------------------\n",
        "        agent.train()\n",
        "        data_size = W.shape[0]\n",
        "        mb = min(int(cfg[\"ppo_minibatch\"]), data_size)\n",
        "\n",
        "        for _ in range(int(cfg[\"ppo_epochs\"])):\n",
        "            perm = torch.randperm(data_size, device=DEVICE)\n",
        "            for start in range(0, data_size, mb):\n",
        "                idx = perm[start:start + mb]\n",
        "\n",
        "                mu, std, v = agent.forward(W[idx])\n",
        "                dist = torch.distributions.Normal(mu, std)\n",
        "\n",
        "                newlp = dist.log_prob(RAW[idx]).sum(dim=1, keepdim=True)\n",
        "                ratio = torch.exp(newlp - OLD_LP[idx])\n",
        "\n",
        "                surr1 = ratio * ADV[idx]\n",
        "                surr2 = torch.clamp(ratio, 1 - cfg[\"ppo_clip_eps\"], 1 + cfg[\"ppo_clip_eps\"]) * ADV[idx]\n",
        "\n",
        "                loss = (-torch.min(surr1, surr2).mean()\n",
        "                        + 0.5 * (v - RET[idx]).pow(2).mean()\n",
        "                        - cfg[\"ppo_entropy_coef\"] * dist.entropy().mean())\n",
        "\n",
        "                opt.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(agent.parameters(), cfg[\"ppo_max_grad_norm\"])\n",
        "                opt.step()\n",
        "\n",
        "        # -----------------------------\n",
        "        # Logging\n",
        "        # -----------------------------\n",
        "        if update_idx % 200 == 0:\n",
        "            print(f\"[PPO] update {update_idx} | episodes_done={episodes_done}/{total_episodes} | rollout_mean_reward={ep_reward:.4f}\")\n",
        "\n",
        "    return agent\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def rollout_ppo_on_noise(cfg: Dict, agent, noise_data):\n",
        "    agent.eval()\n",
        "    dt_local = cfg[\"dt\"]\n",
        "    delay_steps = int(round(cfg[\"tau\"] / dt_local))\n",
        "    window = delay_steps + 2\n",
        "    input_dim = 7\n",
        "\n",
        "    eta_vec = torch.tensor(cfg[\"eta\"], device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "    r_curr = torch.tensor([[cfg[\"S0\"], cfg[\"I0\"], cfg[\"C0\"], cfg[\"R0\"]]], device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "    real_history = [r_curr]\n",
        "    win = torch.zeros(1, window, input_dim, device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "\n",
        "    I_traj = [cfg[\"I0\"]]; C_traj = [cfg[\"C0\"]]\n",
        "    u1_traj, u2_traj = [], []\n",
        "\n",
        "    for t in range(cfg[\"N\"]):\n",
        "        u, _ = agent.act(win, deterministic=True)\n",
        "        u1_t, u2_t = float(u[0, 0].item()), float(u[0, 1].item())\n",
        "\n",
        "        w_t = noise_data[t]\n",
        "        r_hist_stack = torch.stack(real_history)\n",
        "        r_next = step_state_stoch(cfg, r_curr, r_hist_stack, u, w_t, eta_vec)\n",
        "        r_curr = r_next\n",
        "        real_history.append(r_curr)\n",
        "\n",
        "        I_traj.append(float(r_curr[0, 1].item()))\n",
        "        C_traj.append(float(r_curr[0, 2].item()))\n",
        "        u1_traj.append(u1_t); u2_traj.append(u2_t)\n",
        "\n",
        "        idx_tau = max(0, t - cfg[\"K_delay\"])\n",
        "        r_tau = real_history[idx_tau]\n",
        "        I_tau = float(r_tau[0, 1].item())\n",
        "        C_tau = float(r_tau[0, 2].item())\n",
        "\n",
        "        if t < cfg[\"N\"] - 1:\n",
        "            new = torch.tensor([[(t + 1) * dt_local,\n",
        "                                 float(r_curr[0, 0]), float(r_curr[0, 1]),\n",
        "                                 float(r_curr[0, 2]), float(r_curr[0, 3]),\n",
        "                                 I_tau, C_tau]], device=DEVICE, dtype=cfg[\"dtype\"]).unsqueeze(1)\n",
        "            win = torch.cat([win[:, 1:], new], dim=1)\n",
        "\n",
        "    return np.array(I_traj), np.array(C_traj), np.array(u1_traj), np.array(u2_traj)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 10) Rollout benchmark fixed controls on same noise\n",
        "# ============================================================\n",
        "@torch.no_grad()\n",
        "def rollout_fixed_controls(cfg: Dict, u1_arr, u2_arr, noise_data):\n",
        "    eta_vec = torch.tensor(cfg[\"eta\"], device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "    r_curr = torch.tensor([[cfg[\"S0\"], cfg[\"I0\"], cfg[\"C0\"], cfg[\"R0\"]]], device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "    real_history = [r_curr]\n",
        "    I_traj = [cfg[\"I0\"]]; C_traj = [cfg[\"C0\"]]\n",
        "    u1_traj, u2_traj = [], []\n",
        "\n",
        "    for t in range(cfg[\"N\"]):\n",
        "        u_t = torch.tensor([[float(u1_arr[t]), float(u2_arr[t])]], device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "        w_t = noise_data[t]\n",
        "        r_hist_stack = torch.stack(real_history)\n",
        "        r_next = step_state_stoch(cfg, r_curr, r_hist_stack, u_t, w_t, eta_vec)\n",
        "\n",
        "        r_curr = r_next\n",
        "        real_history.append(r_curr)\n",
        "\n",
        "        I_traj.append(float(r_curr[0, 1].item()))\n",
        "        C_traj.append(float(r_curr[0, 2].item()))\n",
        "        u1_traj.append(float(u_t[0, 0].item()))\n",
        "        u2_traj.append(float(u_t[0, 1].item()))\n",
        "\n",
        "    return np.array(I_traj), np.array(C_traj), np.array(u1_traj), np.array(u2_traj)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 11) Evaluation\n",
        "# ============================================================\n",
        "def evaluate_models(cfg: Dict) -> Dict:\n",
        "    set_seed(cfg[\"seed\"])\n",
        "\n",
        "    # Common noise\n",
        "    common_noise = torch.randn(cfg[\"N\"], 1, 4, device=DEVICE, dtype=cfg[\"dtype\"])\n",
        "    t_grid = np.linspace(0, cfg[\"T\"], cfg[\"N\"] + 1)\n",
        "    t_u = t_grid[:-1]\n",
        "\n",
        "    # 1) Benchmark (Adjoint FBSM)\n",
        "    print(\"\\n[Validation] 1. Benchmark (Adjoint)\")\n",
        "    u1_bench, u2_bench, _, _, _, _ = solve_fbsm_benchmark(cfg, common_noise, verbose=True)\n",
        "\n",
        "    # 2) PG-DPO policy training\n",
        "    print(\"\\n[Validation] 2. LSTM-DPO Training\")\n",
        "    policy = LSTMPolicy(hidden=cfg[\"pg_hidden\"]).to(DEVICE)\n",
        "    train_pg_dpo(cfg, policy)\n",
        "\n",
        "    # 3) P-PGDPO style MPC validation\n",
        "    print(\"\\n[Validation] 3. PGDPO (Stepwise MPC)\")\n",
        "    I_mpc, C_mpc, u1_mpc, u2_mpc = run_stepwise_mpc_simulation(\n",
        "        cfg, policy, common_noise, num_mc=cfg[\"mpc_num_mc\"], seed_base=cfg[\"mpc_seed_base\"]\n",
        "    )\n",
        "\n",
        "    # 4) Pure network rollout\n",
        "    print(\"\\n[Validation] 4. LSTM-DPO (Pure rollout)\")\n",
        "    I_net, C_net, u1_net, u2_net = rollout_policy_on_noise(cfg, policy, common_noise)\n",
        "\n",
        "    # 5) PPO\n",
        "    print(\"\\n[Validation] 5. PPO\")\n",
        "    ppo_agent = ppo_train(cfg)\n",
        "    I_ppo, C_ppo, u1_ppo, u2_ppo = rollout_ppo_on_noise(cfg, ppo_agent, common_noise)\n",
        "\n",
        "    # 6) FBSDE\n",
        "    print(\"\\n[Validation] 6. Deep ABSDE (FBSDE-Net)\")\n",
        "    fbsde_net = train_fbsde(cfg)\n",
        "    I_fbsde, C_fbsde, u1_fbsde, u2_fbsde = rollout_fbsde_on_noise(cfg, fbsde_net, common_noise)\n",
        "\n",
        "    # Re-rollout benchmark controls on same noise\n",
        "    I_bench_path, C_bench_path, u1_bench_path, u2_bench_path = rollout_fixed_controls(cfg, u1_bench, u2_bench, common_noise)\n",
        "\n",
        "    # Objectives (cumulative cost curves)\n",
        "    J_bench_cum = cumulative_objective_np(cfg, I_bench_path, C_bench_path, u1_bench_path, u2_bench_path)\n",
        "    J_mpc_cum   = cumulative_objective_np(cfg, I_mpc,       C_mpc,       u1_mpc,       u2_mpc)\n",
        "    J_net_cum   = cumulative_objective_np(cfg, I_net,       C_net,       u1_net,       u2_net)\n",
        "    J_ppo_cum   = cumulative_objective_np(cfg, I_ppo,       C_ppo,       u1_ppo,       u2_ppo)\n",
        "    J_fbsde_cum = cumulative_objective_np(cfg, I_fbsde,     C_fbsde,     u1_fbsde,     u2_fbsde)\n",
        "\n",
        "    # Controls are length N, benchmark arrays are length N+1; align to N\n",
        "    gt_u1 = u1_bench[:cfg[\"N\"]]\n",
        "    gt_u2 = u2_bench[:cfg[\"N\"]]\n",
        "\n",
        "    return {\n",
        "        \"common_noise\": common_noise,\n",
        "        \"t_grid\": t_grid,\n",
        "        \"t_u\": t_u,\n",
        "        \"gt_u1\": gt_u1,\n",
        "        \"gt_u2\": gt_u2,\n",
        "\n",
        "        \"bench\": {\"I\": I_bench_path, \"C\": C_bench_path, \"u1\": u1_bench_path, \"u2\": u2_bench_path, \"J\": J_bench_cum},\n",
        "        \"net\":   {\"I\": I_net,       \"C\": C_net,       \"u1\": u1_net,       \"u2\": u2_net,       \"J\": J_net_cum},\n",
        "        \"mpc\":   {\"I\": I_mpc,       \"C\": C_mpc,       \"u1\": u1_mpc,       \"u2\": u2_mpc,       \"J\": J_mpc_cum},\n",
        "        \"ppo\":   {\"I\": I_ppo,       \"C\": C_ppo,       \"u1\": u1_ppo,       \"u2\": u2_ppo,       \"J\": J_ppo_cum},\n",
        "        \"absde\": {\"I\": I_fbsde,     \"C\": C_fbsde,     \"u1\": u1_fbsde,     \"u2\": u2_fbsde,     \"J\": J_fbsde_cum},\n",
        "    }\n",
        "\n",
        "def plot_results(cfg: Dict, res: Dict, plot_ppo: bool = True, plot_absde: bool = True) -> None:\n",
        "    t_grid = res[\"t_grid\"]\n",
        "    t_u = res[\"t_u\"]\n",
        "    gt_u1 = res[\"gt_u1\"]\n",
        "    gt_u2 = res[\"gt_u2\"]\n",
        "\n",
        "    # u1\n",
        "    plot_series(\n",
        "        t_u, gt_u1,\n",
        "        series={\n",
        "            \"LSTM-DPO\": (res[\"net\"][\"u1\"], \"b:\"),\n",
        "            \"DEEP ABSDE\": (res[\"absde\"][\"u1\"], \"g-.\"),\n",
        "            \"PGDPO\": (res[\"mpc\"][\"u1\"], \"r--\"),\n",
        "            \"PPO\": (res[\"ppo\"][\"u1\"], \"m-\"),\n",
        "        },\n",
        "        title=\"Control u1 (Vaccination)\",\n",
        "        xlabel=\"Time\",\n",
        "        ylabel=\"Vaccination\",\n",
        "        filename_pdf=\"Optimal_Vaccination_u1.pdf\",\n",
        "        ylim=(-0.1, 1.1),\n",
        "        ppo=plot_ppo,\n",
        "        absde=plot_absde,\n",
        "    )\n",
        "\n",
        "    # u2\n",
        "    plot_series(\n",
        "        t_u, gt_u2,\n",
        "        series={\n",
        "            \"LSTM-DPO\": (res[\"net\"][\"u2\"], \"b:\"),\n",
        "            \"DEEP ABSDE\": (res[\"absde\"][\"u2\"], \"g-.\"),\n",
        "            \"PGDPO\": (res[\"mpc\"][\"u2\"], \"r--\"),\n",
        "            \"PPO\": (res[\"ppo\"][\"u2\"], \"m-\"),\n",
        "        },\n",
        "        title=\"Control u2 (Treatment)\",\n",
        "        xlabel=\"Time\",\n",
        "        ylabel=\"Treatment\",\n",
        "        filename_pdf=\"Optimal_Vaccination_u2.pdf\",\n",
        "        ylim=(-0.1, 1.1),\n",
        "        ppo=plot_ppo,\n",
        "        absde=plot_absde,\n",
        "    )\n",
        "\n",
        "    # cumulative cost (N points)\n",
        "    plot_series(\n",
        "        t_u, res[\"bench\"][\"J\"],\n",
        "        series={\n",
        "            \"LSTM-DPO\": (res[\"net\"][\"J\"], \"b:\"),\n",
        "            \"DEEP ABSDE\": (res[\"absde\"][\"J\"], \"g-.\"),\n",
        "            \"PGDPO\": (res[\"mpc\"][\"J\"], \"r--\"),\n",
        "            \"PPO\": (res[\"ppo\"][\"J\"], \"m-\"),\n",
        "        },\n",
        "        title=\"Cumulative Cost J\",\n",
        "        xlabel=\"Time\",\n",
        "        ylabel=\"Cumulative Cost\",\n",
        "        filename_pdf=\"Optimal_Vaccination_J.pdf\",\n",
        "        ylim=None,\n",
        "        ppo=plot_ppo,\n",
        "        absde=plot_absde,\n",
        "    )\n",
        "\n",
        "    # states I\n",
        "    plot_series(\n",
        "        t_grid, res[\"bench\"][\"I\"],\n",
        "        series={\n",
        "            \"LSTM-DPO\": (res[\"net\"][\"I\"], \"b:\"),\n",
        "            \"DEEP ABSDE\": (res[\"absde\"][\"I\"], \"g-.\"),\n",
        "            \"PGDPO\": (res[\"mpc\"][\"I\"], \"r--\"),\n",
        "            \"PPO\": (res[\"ppo\"][\"I\"], \"m-\"),\n",
        "        },\n",
        "        title=\"Infected I Trajectory\",\n",
        "        xlabel=\"Time\",\n",
        "        ylabel=\"Infected I\",\n",
        "        filename_pdf=\"Optimal_Vaccination_I.pdf\",\n",
        "        ylim=None,\n",
        "        ppo=plot_ppo,\n",
        "        absde=plot_absde,\n",
        "    )\n",
        "\n",
        "    # states C\n",
        "    plot_series(\n",
        "        t_grid, res[\"bench\"][\"C\"],\n",
        "        series={\n",
        "            \"LSTM-DPO\": (res[\"net\"][\"C\"], \"b:\"),\n",
        "            \"DEEP ABSDE\": (res[\"absde\"][\"C\"], \"g-.\"),\n",
        "            \"PGDPO\": (res[\"mpc\"][\"C\"], \"r--\"),\n",
        "            \"PPO\": (res[\"ppo\"][\"C\"], \"m-\"),\n",
        "        },\n",
        "        title=\"Chronic C Trajectory\",\n",
        "        xlabel=\"Time\",\n",
        "        ylabel=\"Chronic C\",\n",
        "        filename_pdf=\"Optimal_Vaccination_C.pdf\",\n",
        "        ylim=None,\n",
        "        ppo=plot_ppo,\n",
        "        absde=plot_absde,\n",
        "    )\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 12) Entry point\n",
        "# ============================================================\n",
        "if __name__ == \"__main__\":\n",
        "    results = evaluate_models(CFG)\n",
        "\n",
        "    plot_results(CFG, results, plot_ppo=False, plot_absde=True)\n",
        "    plot_results(CFG, results, plot_ppo=True, plot_absde=False)\n"
      ],
      "metadata": {
        "id": "GGYzAyp7J2LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "svrjqwWxKXfM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}