{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 0. Matplotlib / Font Settings (Type-3 font prevention)\n",
        "# ============================================================\n",
        "plt.rcParams.update({\n",
        "    \"pdf.fonttype\": 42,\n",
        "    \"ps.fonttype\": 42,\n",
        "    \"font.family\": \"serif\",\n",
        "    \"font.serif\": [\"Liberation Serif\", \"FreeSerif\", \"serif\"],\n",
        "    \"font.size\": 10,\n",
        "    \"axes.labelsize\": 10,\n",
        "    \"legend.fontsize\": 9,\n",
        "    \"xtick.labelsize\": 10,\n",
        "    \"ytick.labelsize\": 10,\n",
        "    \"mathtext.fontset\": \"stix\",\n",
        "})\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1. Unified Configuration\n",
        "# ============================================================\n",
        "CFG: Dict = {\n",
        "    # reproducibility / device / dtype\n",
        "    \"seed\": 42,\n",
        "    \"device\": \"cuda:2\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"dtype\": torch.float32,\n",
        "\n",
        "    # dynamics / cost\n",
        "    \"a0\": -0.5,\n",
        "    \"b0\": 1.0,\n",
        "    \"b1\": 0.8,\n",
        "    \"sigma\": 0.2,\n",
        "    \"R\": 1.0,\n",
        "    \"S0\": -10.0,\n",
        "\n",
        "    # time grid\n",
        "    \"T\": 2.0,\n",
        "    \"N_steps\": 20,\n",
        "    \"delta\": 1.0,\n",
        "\n",
        "    # initial condition\n",
        "    \"x0\": 0.0,\n",
        "\n",
        "    # Algo 1 (Warmup / LSTM-DPO)\n",
        "    \"batch_size\": 256,\n",
        "    \"warmup_iters\": 10000,\n",
        "    \"lr\": 1e-4,\n",
        "\n",
        "    # Algo 2 (MV-FABSDE)\n",
        "    \"fbsde_iters\": 10000,\n",
        "    \"fbsde_lr\": 5e-4,\n",
        "    \"fbsde_decay_step\": 10000,\n",
        "\n",
        "    # Stage2 projection (P-PGDPO projection)\n",
        "    \"N_mc_stage2\": 8,\n",
        "    \"N_branch\": 10,\n",
        "    \"N_compare\": 51,\n",
        "\n",
        "    # PPO\n",
        "    \"ppo_hidden\": 64,\n",
        "    \"ppo_optimizer_lr\": 5e-4,\n",
        "    \"ppo_iters\": 1000,\n",
        "    \"ppo_epochs\": 10,\n",
        "    \"ppo_gamma_rl\": 1.0,\n",
        "    \"ppo_gae_lambda\": 0.95,\n",
        "    \"ppo_clip_eps\": 0.2,\n",
        "    \"ppo_minibatch_size\": 512,\n",
        "    \"ppo_entropy_coef\": 0.01,\n",
        "    \"ppo_value_coef\": 0.5,\n",
        "    \"ppo_max_grad_norm\": 0.5,\n",
        "\n",
        "    \"ppo_x_norm\": 2.0,\n",
        "    \"ppo_v_norm\": 5.0,\n",
        "    \"ppo_reward_scale\": 0.1,\n",
        "}\n",
        "\n",
        "DEVICE = torch.device(CFG[\"device\"])\n",
        "torch.set_default_dtype(CFG[\"dtype\"])\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2. Utilities\n",
        "# ============================================================\n",
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def dt(cfg: Dict) -> float:\n",
        "    return cfg[\"T\"] / cfg[\"N_steps\"]\n",
        "\n",
        "\n",
        "def delay_steps(cfg: Dict) -> int:\n",
        "    d = int(round(cfg[\"delta\"] / dt(cfg)))\n",
        "    return max(1, d)\n",
        "\n",
        "\n",
        "def make_time_norm(n: int, cfg: Dict, batch: int, device: torch.device) -> torch.Tensor:\n",
        "    t = n * dt(cfg)\n",
        "    return torch.full((batch,), t / cfg[\"T\"], device=device)\n",
        "\n",
        "\n",
        "def drift_x(x: torch.Tensor, v: torch.Tensor, v_delayed: torch.Tensor, cfg: Dict) -> torch.Tensor:\n",
        "    return cfg[\"a0\"] * x + cfg[\"b0\"] * v + cfg[\"b1\"] * v_delayed\n",
        "\n",
        "\n",
        "def get_v_delayed(v_hist: torch.Tensor, n: int, D: int, device: torch.device) -> torch.Tensor:\n",
        "    if n - D >= 0:\n",
        "        return v_hist[:, n - D]\n",
        "    return torch.zeros(v_hist.size(0), device=device, dtype=v_hist.dtype)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3. Models\n",
        "# ============================================================\n",
        "class LSTMPolicy(nn.Module):\n",
        "    def __init__(self, input_size: int = 2, hidden_size: int = 64):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTMCell(input_size, hidden_size)\n",
        "        self.head = nn.Linear(hidden_size, 1)\n",
        "        with torch.no_grad():\n",
        "            self.head.bias.fill_(0.5)\n",
        "\n",
        "    def init_hidden(self, batch_size: int, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        h0 = torch.zeros(batch_size, self.hidden_size, device=device)\n",
        "        c0 = torch.zeros(batch_size, self.hidden_size, device=device)\n",
        "        return h0, c0\n",
        "\n",
        "    def forward_step(\n",
        "        self, t_norm: torch.Tensor, x_t: torch.Tensor, h: torch.Tensor, c: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        inp = torch.stack([t_norm, x_t], dim=-1)\n",
        "        h_next, c_next = self.lstm(inp, (h, c))\n",
        "        v_raw = self.head(h_next).squeeze(-1)\n",
        "        v = torch.nn.functional.softplus(v_raw)\n",
        "        return v, h_next, c_next\n",
        "\n",
        "\n",
        "class FBSDENet(nn.Module):\n",
        "    def __init__(self, input_size: int = 2, hidden_size: int = 64):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTMCell(input_size, hidden_size)\n",
        "        self.head_Y = nn.Linear(hidden_size, 1)\n",
        "        self.head_Z = nn.Linear(hidden_size, 1)\n",
        "        self.head_EY = nn.Linear(hidden_size, 1)\n",
        "        with torch.no_grad():\n",
        "            self.head_Y.bias.fill_(-5.0)\n",
        "\n",
        "    def init_hidden(self, batch_size: int, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        h0 = torch.zeros(batch_size, self.hidden_size, device=device)\n",
        "        c0 = torch.zeros(batch_size, self.hidden_size, device=device)\n",
        "        return h0, c0\n",
        "\n",
        "    def forward_step(\n",
        "        self, t_norm: torch.Tensor, x_t: torch.Tensor, h: torch.Tensor, c: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        inp = torch.stack([t_norm, x_t], dim=-1)\n",
        "        h_next, c_next = self.lstm(inp, (h, c))\n",
        "        y = self.head_Y(h_next).squeeze(-1)\n",
        "        z = self.head_Z(h_next).squeeze(-1)\n",
        "        ey = self.head_EY(h_next).squeeze(-1)\n",
        "        return y, z, ey, h_next, c_next\n",
        "\n",
        "\n",
        "class WindowPPOAgent(nn.Module):\n",
        "    def __init__(self, input_dim: int = 3, hidden_dim: int = 64, window_size: int = 10):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.actor_mu = nn.Linear(hidden_dim, 1)\n",
        "        self.actor_log_std = nn.Parameter(torch.zeros(1))\n",
        "        self.critic = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        nn.init.constant_(self.actor_mu.bias, 1.5)\n",
        "        for name, param in self.named_parameters():\n",
        "            if \"weight\" in name:\n",
        "                nn.init.orthogonal_(param, gain=1.0)\n",
        "\n",
        "    def forward(self, x_window: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        out, _ = self.lstm(x_window)\n",
        "        last_hidden = out[:, -1, :]\n",
        "        mu = self.actor_mu(last_hidden)\n",
        "        val = self.critic(last_hidden)\n",
        "        std = torch.exp(self.actor_log_std).expand_as(mu)\n",
        "        return mu, std, val\n",
        "\n",
        "    def get_action(self, x_window: torch.Tensor, deterministic: bool = False):\n",
        "        mu, std, val = self.forward(x_window)\n",
        "        if deterministic:\n",
        "            action_raw = mu\n",
        "            action = torch.nn.functional.softplus(action_raw) + 1e-6\n",
        "            return action, val\n",
        "\n",
        "        dist = torch.distributions.Normal(mu, std)\n",
        "        action_raw = dist.sample()\n",
        "        action = torch.nn.functional.softplus(action_raw) + 1e-6\n",
        "        log_prob = dist.log_prob(action_raw)\n",
        "        return action, log_prob, val, action_raw\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4. Algorithm 1: Warmup (LSTM-DPO)\n",
        "# ============================================================\n",
        "def simulate_batch_episode_pg(policy: LSTMPolicy, cfg: Dict, detach_policy: bool = False) -> torch.Tensor:\n",
        "    device = DEVICE\n",
        "    N, D = cfg[\"N_steps\"], delay_steps(cfg)\n",
        "    batch = cfg[\"batch_size\"]\n",
        "\n",
        "    x = torch.full((batch,), cfg[\"x0\"], device=device)\n",
        "    v_hist = torch.zeros(batch, N + 1, device=device)\n",
        "\n",
        "    h, c = policy.init_hidden(batch, device)\n",
        "    cost = torch.zeros(batch, device=device)\n",
        "\n",
        "    for n in range(N):\n",
        "        t_norm = make_time_norm(n, cfg, batch, device)\n",
        "        if detach_policy:\n",
        "            with torch.no_grad():\n",
        "                v, h, c = policy.forward_step(t_norm, x, h, c)\n",
        "        else:\n",
        "            v, h, c = policy.forward_step(t_norm, x, h, c)\n",
        "\n",
        "        v_hist[:, n] = v\n",
        "        v_del = get_v_delayed(v_hist, n, D, device)\n",
        "\n",
        "        dB = math.sqrt(dt(cfg)) * torch.randn(batch, device=device)\n",
        "        x = x + drift_x(x, v, v_del, cfg) * dt(cfg) + cfg[\"sigma\"] * dB\n",
        "        cost = cost + cfg[\"R\"] * v**2 * dt(cfg)\n",
        "\n",
        "    cost = cost + cfg[\"S0\"] * x\n",
        "    return cost.mean()\n",
        "\n",
        "\n",
        "def warmup_train(policy: LSTMPolicy, cfg: Dict) -> LSTMPolicy:\n",
        "    policy.to(DEVICE)\n",
        "    policy.train()\n",
        "    opt = optim.Adam(policy.parameters(), lr=cfg[\"lr\"])\n",
        "\n",
        "    for it in range(cfg[\"warmup_iters\"]):\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        J = simulate_batch_episode_pg(policy, cfg, detach_policy=False)\n",
        "        J.backward()\n",
        "        opt.step()\n",
        "        if (it + 1) % 200 == 0:\n",
        "            print(f\"[Warmup] iter={it+1}, J={J.item():.4f}\")\n",
        "\n",
        "    return policy\n",
        "\n",
        "\n",
        "def rollout_policy_pg_deterministic(policy: LSTMPolicy, cfg: Dict) -> Dict[str, List[torch.Tensor]]:\n",
        "    device = DEVICE\n",
        "    T, N, D = cfg[\"T\"], cfg[\"N_steps\"], delay_steps(cfg)\n",
        "    policy.eval()\n",
        "\n",
        "    x = torch.full((1,), cfg[\"x0\"], device=device)\n",
        "    v_hist = torch.zeros(1, N + 1, device=device)\n",
        "    h, c = policy.init_hidden(1, device)\n",
        "\n",
        "    xs, vs, hs, cs, v_hists = [], [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for n in range(N):\n",
        "            xs.append(x.clone())\n",
        "            hs.append(h.clone())\n",
        "            cs.append(c.clone())\n",
        "            v_hists.append(v_hist.clone())\n",
        "\n",
        "            t_norm = torch.full((1,), (n * dt(cfg)) / T, device=device)\n",
        "            v, h, c = policy.forward_step(t_norm, x, h, c)\n",
        "            vs.append(v.clone())\n",
        "\n",
        "            v_hist[:, n] = v\n",
        "            v_del = get_v_delayed(v_hist, n, D, device)\n",
        "            x = x + drift_x(x, v, v_del, cfg) * dt(cfg)\n",
        "\n",
        "        xs.append(x.clone())\n",
        "\n",
        "    return {\"xs\": xs, \"vs\": vs, \"hs\": hs, \"cs\": cs, \"v_hists\": v_hists}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5. Algorithm 2: MV-FABSDE\n",
        "# ============================================================\n",
        "def train_fbsde(net: FBSDENet, cfg: Dict) -> FBSDENet:\n",
        "    net.to(DEVICE)\n",
        "    net.train()\n",
        "\n",
        "    opt = optim.Adam(net.parameters(), lr=cfg[\"fbsde_lr\"])\n",
        "    scheduler = optim.lr_scheduler.StepLR(opt, step_size=cfg[\"fbsde_decay_step\"], gamma=0.1)\n",
        "    mse = nn.MSELoss()\n",
        "\n",
        "    N, D = cfg[\"N_steps\"], delay_steps(cfg)\n",
        "    batch = cfg[\"batch_size\"]\n",
        "    T = cfg[\"T\"]\n",
        "\n",
        "    for it in range(cfg[\"fbsde_iters\"]):\n",
        "        dB = torch.randn(batch, N, device=DEVICE) * math.sqrt(dt(cfg))\n",
        "\n",
        "        x = torch.full((batch,), cfg[\"x0\"], device=DEVICE)\n",
        "        v_hist = torch.zeros(batch, N + 1, device=DEVICE)\n",
        "        h, c = net.init_hidden(batch, DEVICE)\n",
        "\n",
        "        Y_pred = [None] * (N + 1)\n",
        "        Z_pred = [None] * (N + 1)\n",
        "        EY_pred = [None] * (N + 1)\n",
        "        tildeY = [None] * (N + 1)\n",
        "\n",
        "        t0 = torch.zeros(batch, device=DEVICE)\n",
        "        y, z, ey, h, c = net.forward_step(t0, x, h, c)\n",
        "        Y_pred[0], Z_pred[0], EY_pred[0] = y, z, ey\n",
        "\n",
        "        for i in range(N):\n",
        "            y_i, z_i, ey_i = Y_pred[i], Z_pred[i], EY_pred[i]\n",
        "            ey_used = ey_i if i <= N - D else torch.zeros_like(ey_i)\n",
        "\n",
        "            v_val = - (cfg[\"b0\"] * y_i + cfg[\"b1\"] * ey_used) / (2.0 * cfg[\"R\"])\n",
        "            v = torch.relu(v_val)\n",
        "            v_hist[:, i] = v\n",
        "\n",
        "            v_del = get_v_delayed(v_hist, i, D, DEVICE)\n",
        "            x = x + drift_x(x, v, v_del, cfg) * dt(cfg) + cfg[\"sigma\"] * dB[:, i]\n",
        "\n",
        "            tildeY[i + 1] = y_i - cfg[\"a0\"] * y_i * dt(cfg) + z_i * dB[:, i]\n",
        "\n",
        "            t_next = torch.full((batch,), ((i + 1) * dt(cfg)) / T, device=DEVICE)\n",
        "            y, z, ey, h, c = net.forward_step(t_next, x, h, c)\n",
        "            Y_pred[i + 1], Z_pred[i + 1], EY_pred[i + 1] = y, z, ey\n",
        "\n",
        "        L1 = 0.0\n",
        "        for i in range(N):\n",
        "            L1 = L1 + mse(Y_pred[i + 1], tildeY[i + 1])\n",
        "        L1 = L1 + mse(Y_pred[N], torch.full((batch,), cfg[\"S0\"], device=DEVICE))\n",
        "\n",
        "        L2 = 0.0\n",
        "        for i in range(0, N - D + 1):\n",
        "            L2 = L2 + mse(EY_pred[i], tildeY[i + D])\n",
        "\n",
        "        L1 = L1 / max(1, N)\n",
        "        L2 = L2 / max(1, N - D + 1)\n",
        "        loss = L1 + L2\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        if (it + 1) % 2000 == 0:\n",
        "            print(f\"[Alg2 MV-FABSDE] iter={it+1}, L1={L1.item():.6f}, L2={L2.item():.6f}, total={loss.item():.6f}\")\n",
        "\n",
        "    return net\n",
        "\n",
        "\n",
        "def rollout_fbsde_deterministic(net: FBSDENet, cfg: Dict) -> List[torch.Tensor]:\n",
        "    net.eval()\n",
        "\n",
        "    N, D = cfg[\"N_steps\"], delay_steps(cfg)\n",
        "    T = cfg[\"T\"]\n",
        "\n",
        "    x = torch.full((1,), cfg[\"x0\"], device=DEVICE)\n",
        "    v_hist = torch.zeros(1, N + 1, device=DEVICE)\n",
        "    h, c = net.init_hidden(1, DEVICE)\n",
        "\n",
        "    vs = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(N):\n",
        "            t_norm = torch.full((1,), (i * dt(cfg)) / T, device=DEVICE)\n",
        "            y, z, ey, h, c = net.forward_step(t_norm, x, h, c)\n",
        "            ey_used = ey if i <= N - D else torch.zeros_like(ey)\n",
        "            v_val = - (cfg[\"b0\"] * y + cfg[\"b1\"] * ey_used) / (2.0 * cfg[\"R\"])\n",
        "            v = torch.relu(v_val)\n",
        "\n",
        "            vs.append(v.clone())\n",
        "            v_hist[:, i] = v\n",
        "            v_del = get_v_delayed(v_hist, i, D, DEVICE)\n",
        "            x = x + drift_x(x, v, v_del, cfg) * dt(cfg)\n",
        "\n",
        "    return vs\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 6. Algorithm 3: PPO\n",
        "# ============================================================\n",
        "def train_ppo_window(cfg: Dict) -> WindowPPOAgent:\n",
        "    N = cfg[\"N_steps\"]\n",
        "    D = delay_steps(cfg)\n",
        "    window = D + 2\n",
        "\n",
        "    x_norm = cfg[\"ppo_x_norm\"]\n",
        "    v_norm = cfg[\"ppo_v_norm\"]\n",
        "    reward_scale = cfg[\"ppo_reward_scale\"]\n",
        "\n",
        "    agent = WindowPPOAgent(input_dim=3, hidden_dim=cfg[\"ppo_hidden\"], window_size=window).to(DEVICE)\n",
        "\n",
        "    optimizer = optim.Adam(agent.parameters(), lr=cfg[\"ppo_optimizer_lr\"])\n",
        "\n",
        "    ppo_iters = cfg[\"ppo_iters\"]\n",
        "    ppo_epochs = cfg[\"ppo_epochs\"]\n",
        "    gamma_rl = cfg[\"ppo_gamma_rl\"]\n",
        "    lam_gae = cfg[\"ppo_gae_lambda\"]\n",
        "    clip_eps = cfg[\"ppo_clip_eps\"]\n",
        "    minibatch_size = cfg[\"ppo_minibatch_size\"]\n",
        "\n",
        "    for it in range(ppo_iters):\n",
        "        batch_windows = []\n",
        "        batch_raw_acts = []\n",
        "        batch_log_probs = []\n",
        "        batch_vals = []\n",
        "        batch_rews = []\n",
        "\n",
        "        B = cfg[\"batch_size\"]\n",
        "        x = torch.full((B, 1), cfg[\"x0\"], device=DEVICE)\n",
        "        v_hist = torch.zeros(B, N + 1, device=DEVICE)\n",
        "\n",
        "        w = torch.zeros(B, window, 3, device=DEVICE)\n",
        "        w[:, :, 0] = 0.0\n",
        "        w[:, :, 1] = cfg[\"x0\"] / x_norm\n",
        "        w[:, :, 2] = 0.0\n",
        "\n",
        "        episode_rew_sum = torch.zeros(B, device=DEVICE)\n",
        "\n",
        "        for n in range(N):\n",
        "            with torch.no_grad():\n",
        "                v, log_prob, val, raw_act = agent.get_action(w, deterministic=False)\n",
        "\n",
        "            batch_windows.append(w.clone())\n",
        "            batch_raw_acts.append(raw_act)\n",
        "            batch_log_probs.append(log_prob)\n",
        "            batch_vals.append(val)\n",
        "\n",
        "            v_flat = v.squeeze(-1)\n",
        "            v_hist[:, n] = v_flat\n",
        "            v_del = get_v_delayed(v_hist, n, D, DEVICE)\n",
        "\n",
        "            dW = math.sqrt(dt(cfg)) * torch.randn(B, device=DEVICE)\n",
        "            x_next = x.squeeze(-1) + drift_x(x.squeeze(-1), v_flat, v_del, cfg) * dt(cfg) + cfg[\"sigma\"] * dW\n",
        "            x = x_next.unsqueeze(-1)\n",
        "\n",
        "            step_reward = -cfg[\"R\"] * (v_flat**2) * dt(cfg)\n",
        "            if n == N - 1:\n",
        "                step_reward = step_reward + (-cfg[\"S0\"] * x_next)\n",
        "            batch_rews.append(step_reward * reward_scale)\n",
        "            episode_rew_sum += step_reward\n",
        "\n",
        "            if n < N - 1:\n",
        "                next_t_norm = ((n + 1) * dt(cfg)) / cfg[\"T\"]\n",
        "                new_in = torch.cat(\n",
        "                    [\n",
        "                        torch.full((B, 1, 1), next_t_norm, device=DEVICE),\n",
        "                        (x / x_norm).unsqueeze(1),\n",
        "                        (v / v_norm).unsqueeze(1),\n",
        "                    ],\n",
        "                    dim=2,\n",
        "                )\n",
        "                w = torch.cat([w[:, 1:, :], new_in], dim=1)\n",
        "\n",
        "        returns = []\n",
        "        gae = torch.zeros(B, device=DEVICE)\n",
        "        for t in reversed(range(N)):\n",
        "            curr_val = batch_vals[t].squeeze(-1)\n",
        "            if t == N - 1:\n",
        "                next_val = torch.zeros_like(curr_val)\n",
        "                nonterm = 0.0\n",
        "            else:\n",
        "                next_val = batch_vals[t + 1].squeeze(-1)\n",
        "                nonterm = 1.0\n",
        "\n",
        "            delta = batch_rews[t] + gamma_rl * next_val * nonterm - curr_val\n",
        "            gae = delta + gamma_rl * lam_gae * nonterm * gae\n",
        "            returns.insert(0, gae + curr_val)\n",
        "\n",
        "        b_windows = torch.stack(batch_windows).view(-1, window, 3)\n",
        "        b_raw_acts = torch.stack(batch_raw_acts).view(-1, 1)\n",
        "        b_log_probs = torch.stack(batch_log_probs).view(-1, 1)\n",
        "        b_rets = torch.stack(returns).view(-1, 1)\n",
        "        b_vals = torch.stack(batch_vals).view(-1, 1)\n",
        "\n",
        "        adv = b_rets - b_vals\n",
        "        adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
        "\n",
        "        dataset = b_windows.size(0)\n",
        "        perm = torch.randperm(dataset, device=DEVICE)\n",
        "\n",
        "        for _ in range(ppo_epochs):\n",
        "            for start in range(0, dataset, minibatch_size):\n",
        "                idx = perm[start: start + minibatch_size]\n",
        "\n",
        "                mu, std, val = agent.forward(b_windows[idx])\n",
        "                dist = torch.distributions.Normal(mu, std)\n",
        "                new_lp = dist.log_prob(b_raw_acts[idx])\n",
        "                entropy = dist.entropy().mean()\n",
        "\n",
        "                ratio = torch.exp(new_lp - b_log_probs[idx])\n",
        "                surr1 = ratio * adv[idx]\n",
        "                surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * adv[idx]\n",
        "\n",
        "                loss = (\n",
        "                    -torch.min(surr1, surr2).mean()\n",
        "                    + cfg[\"ppo_value_coef\"] * ((val - b_rets[idx]) ** 2).mean()\n",
        "                    - cfg[\"ppo_entropy_coef\"] * entropy\n",
        "                )\n",
        "\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(agent.parameters(), cfg[\"ppo_max_grad_norm\"])\n",
        "                optimizer.step()\n",
        "\n",
        "        if (it + 1) % 100 == 0:\n",
        "            print(f\"[PPO] iter={it+1}, AvgReward={episode_rew_sum.mean().item():.4f}\")\n",
        "\n",
        "    return agent\n",
        "\n",
        "\n",
        "def rollout_ppo_deterministic(agent: WindowPPOAgent, cfg: Dict) -> np.ndarray:\n",
        "    agent.eval()\n",
        "\n",
        "    N, D = cfg[\"N_steps\"], delay_steps(cfg)\n",
        "    window = D + 2\n",
        "    x_norm, v_norm = cfg[\"ppo_x_norm\"], cfg[\"ppo_v_norm\"]\n",
        "\n",
        "    x = torch.full((1, 1), cfg[\"x0\"], device=DEVICE)\n",
        "    v_hist = torch.zeros(1, N + 1, device=DEVICE)\n",
        "\n",
        "    w = torch.zeros(1, window, 3, device=DEVICE)\n",
        "    w[:, :, 0] = 0.0\n",
        "    w[:, :, 1] = cfg[\"x0\"] / x_norm\n",
        "    w[:, :, 2] = 0.0\n",
        "\n",
        "    vs = []\n",
        "    with torch.no_grad():\n",
        "        for n in range(N):\n",
        "            v, _ = agent.get_action(w, deterministic=True)\n",
        "            v_val = v.item()\n",
        "            vs.append(v_val)\n",
        "\n",
        "            v_hist[:, n] = v_val\n",
        "            v_del = get_v_delayed(v_hist, n, D, DEVICE)\n",
        "\n",
        "            x = x + drift_x(x, v, v_del, cfg) * dt(cfg)\n",
        "\n",
        "            if n < N - 1:\n",
        "                next_t_norm = ((n + 1) * dt(cfg)) / cfg[\"T\"]\n",
        "                new_in = torch.cat(\n",
        "                    [\n",
        "                        torch.full((1, 1, 1), next_t_norm, device=DEVICE),\n",
        "                        (x / x_norm).unsqueeze(1),\n",
        "                        (v / v_norm).unsqueeze(1),\n",
        "                    ],\n",
        "                    dim=2,\n",
        "                )\n",
        "                w = torch.cat([w[:, 1:, :], new_in], dim=1)\n",
        "\n",
        "    return np.array(vs)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 7. Benchmark\n",
        "# ============================================================\n",
        "def analytic_solution(cfg: Dict) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    T, N = cfg[\"T\"], cfg[\"N_steps\"]\n",
        "    a0, b0, b1 = cfg[\"a0\"], cfg[\"b0\"], cfg[\"b1\"]\n",
        "    R, S0 = cfg[\"R\"], cfg[\"S0\"]\n",
        "    D = delay_steps(cfg)\n",
        "\n",
        "    t_grid = np.linspace(0, T, N + 1)\n",
        "    y = S0 * np.exp(a0 * (T - t_grid))\n",
        "\n",
        "    u = np.zeros_like(y)\n",
        "    for i in range(N + 1):\n",
        "        term1 = b0 * y[i]\n",
        "        term2 = b1 * y[i + D] if (i + D) <= N else 0.0\n",
        "        u[i] = max(0.0, - (term1 + term2) / (2.0 * R))\n",
        "\n",
        "    return t_grid, u\n",
        "\n",
        "\n",
        "def mae_rmse(y_hat: np.ndarray, y_ref: np.ndarray) -> Tuple[float, float]:\n",
        "    y_hat = np.asarray(y_hat).reshape(-1)\n",
        "    y_ref = np.asarray(y_ref).reshape(-1)\n",
        "    err = y_hat - y_ref\n",
        "    mae = float(np.mean(np.abs(err)))\n",
        "    rmse = float(np.sqrt(np.mean(err**2)))\n",
        "    return mae, rmse\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 8. P-PGDPO Projection\n",
        "# ============================================================\n",
        "def estimate_costate_mc(\n",
        "    policy: LSTMPolicy,\n",
        "    cfg: Dict,\n",
        "    n0: int,\n",
        "    x0_val: torch.Tensor,\n",
        "    h0: torch.Tensor,\n",
        "    c0: torch.Tensor,\n",
        "    v_hist0: torch.Tensor,\n",
        "    M: int,\n",
        ") -> torch.Tensor:\n",
        "    policy.eval()\n",
        "\n",
        "    N, D = cfg[\"N_steps\"], delay_steps(cfg)\n",
        "    T = cfg[\"T\"]\n",
        "\n",
        "    lambdas = []\n",
        "    for _ in range(M):\n",
        "        x_init = x0_val.clone().detach().requires_grad_(True)\n",
        "        x = x_init\n",
        "\n",
        "        h = h0.clone().detach()\n",
        "        c = c0.clone().detach()\n",
        "        v_hist = v_hist0.clone().detach()\n",
        "\n",
        "        cost = torch.zeros(1, device=DEVICE)\n",
        "        for n in range(n0, N):\n",
        "            t_norm = torch.full((1,), (n * dt(cfg)) / T, device=DEVICE)\n",
        "            v, h, c = policy.forward_step(t_norm, x, h, c)\n",
        "\n",
        "            v_hist[:, n] = v\n",
        "            v_del = get_v_delayed(v_hist, n, D, DEVICE)\n",
        "\n",
        "            dB = math.sqrt(dt(cfg)) * torch.randn(1, device=DEVICE)\n",
        "            x = x + drift_x(x, v, v_del, cfg) * dt(cfg) + cfg[\"sigma\"] * dB\n",
        "            cost = cost + cfg[\"R\"] * v**2 * dt(cfg)\n",
        "\n",
        "        cost = cost + cfg[\"S0\"] * x\n",
        "        (grad_x,) = torch.autograd.grad(cost, x_init, retain_graph=False, create_graph=False)\n",
        "        lambdas.append(grad_x.detach())\n",
        "\n",
        "    return torch.mean(torch.stack(lambdas, dim=0), dim=0)\n",
        "\n",
        "\n",
        "def project_p_pgdpo(\n",
        "    policy_pg: LSTMPolicy,\n",
        "    cfg: Dict,\n",
        "    t_grid: np.ndarray,\n",
        "    rollout_pg: Dict[str, List[torch.Tensor]],\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    N = cfg[\"N_steps\"]\n",
        "    D = delay_steps(cfg)\n",
        "\n",
        "    idx_list = sorted(\n",
        "        set(\n",
        "            int(round(i * (N - 1) / (cfg[\"N_compare\"] - 1)))\n",
        "            for i in range(cfg[\"N_compare\"])\n",
        "        )\n",
        "    )\n",
        "\n",
        "    xs = rollout_pg[\"xs\"]\n",
        "    hs = rollout_pg[\"hs\"]\n",
        "    cs = rollout_pg[\"cs\"]\n",
        "    v_hists = rollout_pg[\"v_hists\"]\n",
        "\n",
        "    v_pmp_list = []\n",
        "    t_cmp = []\n",
        "\n",
        "    for n in idx_list:\n",
        "        lam_t = estimate_costate_mc(\n",
        "            policy_pg, cfg, n, xs[n], hs[n], cs[n], v_hists[n], M=cfg[\"N_mc_stage2\"]\n",
        "        ).item()\n",
        "\n",
        "        if n + D >= N:\n",
        "            lam_t_delta = cfg[\"S0\"] if (n + D) == N else 0.0\n",
        "        else:\n",
        "            future = []\n",
        "            for _ in range(cfg[\"N_branch\"]):\n",
        "                x_curr = xs[n].clone()\n",
        "                h_curr = hs[n].clone()\n",
        "                c_curr = cs[n].clone()\n",
        "                v_hist_curr = v_hists[n].clone()\n",
        "\n",
        "                for k in range(n, n + D):\n",
        "                    t_k_norm = torch.full((1,), (k * dt(cfg)) / cfg[\"T\"], device=DEVICE)\n",
        "                    with torch.no_grad():\n",
        "                        v, h_curr, c_curr = policy_pg.forward_step(t_k_norm, x_curr, h_curr, c_curr)\n",
        "\n",
        "                    v_hist_curr[:, k] = v\n",
        "                    v_del = get_v_delayed(v_hist_curr, k, D, DEVICE)\n",
        "                    dB = math.sqrt(dt(cfg)) * torch.randn(1, device=DEVICE)\n",
        "                    x_curr = x_curr + drift_x(x_curr, v, v_del, cfg) * dt(cfg) + cfg[\"sigma\"] * dB\n",
        "\n",
        "                lam_future = estimate_costate_mc(\n",
        "                    policy_pg, cfg, n + D, x_curr, h_curr, c_curr, v_hist_curr, M=cfg[\"N_mc_stage2\"]\n",
        "                ).item()\n",
        "                future.append(lam_future)\n",
        "\n",
        "            lam_t_delta = float(np.mean(future))\n",
        "\n",
        "        val = - (cfg[\"b0\"] * lam_t + cfg[\"b1\"] * lam_t_delta) / (2.0 * cfg[\"R\"])\n",
        "        v_pmp_list.append(max(0.0, val))\n",
        "        t_cmp.append(t_grid[n])\n",
        "\n",
        "    return np.array(t_cmp), np.array(v_pmp_list)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 9. Plotting\n",
        "# ============================================================\n",
        "def plot_controls(\n",
        "    t_grid: np.ndarray,\n",
        "    gt: np.ndarray,\n",
        "    c_pg: np.ndarray,\n",
        "    c_fbsde: np.ndarray,\n",
        "    c_ppo: np.ndarray,\n",
        "    c_proj: np.ndarray,\n",
        "    ppo: bool = True,\n",
        "    fbsde: bool = True,\n",
        ") -> None:\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    plt.plot(t_grid, gt, \"k-\", lw=2, label=\"Benchmark\")\n",
        "\n",
        "    plt.plot(t_grid, c_pg, \"b:\", label=\"LSTM-DPO\")\n",
        "    if fbsde:\n",
        "        plt.plot(t_grid, c_fbsde, \"g-.\", label=\"DEEP ABSDE\")\n",
        "    if ppo:\n",
        "        plt.plot(t_grid, c_ppo, \"m-\", label=\"PPO\")\n",
        "    plt.plot(t_grid, c_proj, \"r--\", label=\"PGDPO\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Advertising Expenditure\")\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.1)\n",
        "    if ppo:\n",
        "        plt.savefig(\"Benchmark2_control_ppo.pdf\", bbox_inches=\"tight\")\n",
        "    else:\n",
        "        plt.savefig(\"Benchmark2_control.pdf\", bbox_inches=\"tight\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 10. Metrics + Plot driver\n",
        "# ============================================================\n",
        "def compute_metrics_and_plot(\n",
        "    policy_pg: LSTMPolicy,\n",
        "    fbsde_net: Optional[FBSDENet],\n",
        "    policy_ppo: WindowPPOAgent,\n",
        "    cfg: Dict,\n",
        ") -> None:\n",
        "    policy_pg.eval()\n",
        "    if fbsde_net is not None:\n",
        "        fbsde_net.eval()\n",
        "    policy_ppo.eval()\n",
        "\n",
        "    # --- benchmark ---\n",
        "    t_grid_full, v_star_full = analytic_solution(cfg)\n",
        "    t_plot = t_grid_full[:-1]\n",
        "    v_star = v_star_full[:-1]\n",
        "\n",
        "\n",
        "    # --- Algo 1 rollout (deterministic) ---\n",
        "    rollout_pg = rollout_policy_pg_deterministic(policy_pg, cfg)\n",
        "    v_pg = np.array([v.item() for v in rollout_pg[\"vs\"]])  # (N,)\n",
        "\n",
        "    # --- Algo 2 rollout (optional) ---\n",
        "    # (kept as-is; metrics printing optional if you still want)\n",
        "    v_fbsde_list = rollout_fbsde_deterministic(fbsde_net, cfg)\n",
        "    v_fbsde = np.array([v.item() for v in v_fbsde_list])  # (N,)\n",
        "\n",
        "    # --- Algo 3 PPO rollout (deterministic) ---\n",
        "    v_ppo = rollout_ppo_deterministic(policy_ppo, cfg)  # (N,)\n",
        "\n",
        "    # --- Projection (P-PGDPO) ---\n",
        "    t_cmp, v_pmp = project_p_pgdpo(policy_pg, cfg, t_grid_full, rollout_pg)\n",
        "\n",
        "    v_star_cmp = np.interp(t_cmp, t_grid_full, v_star_full)\n",
        "    v_pg_cmp = np.interp(t_cmp, t_grid_full[:-1], v_pg)\n",
        "    v_ppo_cmp = np.interp(t_cmp, t_grid_full[:-1], v_ppo)\n",
        "    v_fbsde_cmp = np.interp(t_cmp, t_grid_full[:-1], v_fbsde)\n",
        "\n",
        "    mae_pg, rmse_pg = mae_rmse(v_pg_cmp, v_star_cmp)\n",
        "    mae_ppo, rmse_ppo = mae_rmse(v_ppo_cmp, v_star_cmp)\n",
        "    mae_pmp, rmse_pmp = mae_rmse(v_pmp, v_star_cmp)\n",
        "    mae_f, rmse_f = mae_rmse(v_fbsde_cmp, v_star_cmp)\n",
        "\n",
        "    print(f\"RMSE & MAE (Algo 1 PG)      : {rmse_pg:.6f}, {mae_pg:.6f}\")\n",
        "    print(f\"RMSE & MAE (Algo 2 FBSDE)   : {rmse_f:.6f}, {mae_f:.6f}\")\n",
        "    print(f\"RMSE & MAE (Algo 3 PPO)     : {rmse_ppo:.6f}, {mae_ppo:.6f}\")\n",
        "    print(f\"RMSE & MAE (P-PGDPO)        : {rmse_pmp:.6f}, {mae_pmp:.6f}\")\n",
        "\n",
        "    # --- plot using your function ---\n",
        "    v_pmp_on_N = np.interp(t_plot, t_cmp, v_pmp)\n",
        "\n",
        "    plot_controls(t_plot, v_star, v_pg, v_fbsde, v_ppo, v_pmp_on_N, ppo=True, fbsde=False)\n",
        "    plot_controls(t_plot, v_star, v_pg, v_fbsde, v_ppo, v_pmp_on_N, ppo=False, fbsde=True)\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 10. Main\n",
        "# ============================================================\n",
        "def main() -> None:\n",
        "    set_seed(CFG[\"seed\"])\n",
        "\n",
        "    policy_pg = LSTMPolicy(hidden_size=CFG[\"ppo_hidden\"])\n",
        "    policy_pg = warmup_train(policy_pg, CFG)\n",
        "\n",
        "    fbsde_net = FBSDENet(hidden_size=CFG[\"ppo_hidden\"])\n",
        "    fbsde_net = train_fbsde(fbsde_net, CFG)\n",
        "\n",
        "    policy_ppo = train_ppo_window(CFG)\n",
        "\n",
        "    compute_metrics_and_plot(policy_pg, fbsde_net, policy_ppo, CFG)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Fp9hvwjYpK6c"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}