{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "import random\n",
        "from typing import Dict, Any, Tuple, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 0. Matplotlib style\n",
        "# =============================================================================\n",
        "plt.rcParams.update({\n",
        "    \"pdf.fonttype\": 42,\n",
        "    \"font.family\": \"serif\",\n",
        "    \"font.serif\": [\"Liberation Serif\", \"FreeSerif\", \"serif\"],\n",
        "    \"font.size\": 10,\n",
        "    \"axes.labelsize\": 10,\n",
        "    \"legend.fontsize\": 9,\n",
        "    \"xtick.labelsize\": 10,\n",
        "    \"ytick.labelsize\": 10,\n",
        "    \"mathtext.fontset\": \"stix\",\n",
        "})\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 1. Unified Configuration\n",
        "# =============================================================================\n",
        "CFG: Dict[str, Any] = {\n",
        "    # reproducibility / device / dtype\n",
        "    \"seed\": 7,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"dtype\": torch.float32,\n",
        "\n",
        "    # dynamics / cost (advertising)\n",
        "    \"a0\": -0.5,\n",
        "    \"b0\": 1.0,\n",
        "    \"sigma\": 0.2,\n",
        "    \"R\": 0.1,\n",
        "    \"S0\": -5.0,\n",
        "\n",
        "    # time grid\n",
        "    \"T\": 5.0,\n",
        "    \"N_steps\": 50,\n",
        "\n",
        "    # distributed delay kernel (theta in [-delta, 0])\n",
        "    \"delta\": 3.0,\n",
        "    \"H_delay\": 20,\n",
        "    \"a1_scale\": -10.0,\n",
        "    \"a1_lambda\": 1.0,\n",
        "\n",
        "    # initial history\n",
        "    \"x0\": 10.0,\n",
        "    \"x_hist_const\": 10.0,\n",
        "\n",
        "    # -----------------------------\n",
        "    # Method 1: LSTM-DPO warm-up\n",
        "    # -----------------------------\n",
        "    \"warmup_batch_size\": 256,\n",
        "    \"warmup_iters\": 10000,\n",
        "    \"lr_pg\": 3e-4,\n",
        "\n",
        "    # -----------------------------\n",
        "    # Stage 2 projection\n",
        "    # -----------------------------\n",
        "    \"N_mc_stage2\": 4096,\n",
        "\n",
        "    # -----------------------------\n",
        "    # Method 2: STANDARD PPO\n",
        "    # -----------------------------\n",
        "    \"ppo_num_envs\": 256,          # number of parallel episodes per rollout/update\n",
        "    \"ppo_lr\": 1e-3,\n",
        "    \"ppo_epochs\": 10,\n",
        "    \"ppo_minibatch_size\": 1024,\n",
        "    \"ppo_gamma\": 1.,\n",
        "    \"ppo_lambda\": 0.95,\n",
        "    \"ppo_clip_eps\": 0.2,\n",
        "    \"ppo_entropy_coef\": 0.01,\n",
        "    \"ppo_value_coef\": 0.5,\n",
        "    \"ppo_max_grad_norm\": 0.5,\n",
        "\n",
        "    # training budget\n",
        "    \"ppo_total_episodes\": 102400, # # of total episodes\n",
        "\n",
        "    # Gaussian policy std\n",
        "    \"ppo_init_log_std\": math.log(0.6),\n",
        "\n",
        "    # network size for PPO\n",
        "    \"ppo_hidden\": 64,\n",
        "}\n",
        "\n",
        "DEVICE = torch.device(CFG[\"device\"])\n",
        "torch.set_default_dtype(CFG[\"dtype\"])\n",
        "\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def get_dt(cfg: Dict[str, Any]) -> float:\n",
        "    return float(cfg[\"T\"]) / float(cfg[\"N_steps\"])\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 2. Physics helpers\n",
        "# =============================================================================\n",
        "def build_a1_kernel_np(cfg: Dict[str, Any]) -> np.ndarray:\n",
        "    H = int(cfg[\"H_delay\"])\n",
        "    delta = float(cfg[\"delta\"])\n",
        "    dtheta = delta / H\n",
        "    theta = -delta + dtheta * (np.arange(H) + 0.5)\n",
        "    a1_vals = float(cfg[\"a1_scale\"]) * np.exp(float(cfg[\"a1_lambda\"]) * theta)\n",
        "    w = a1_vals * dtheta\n",
        "    w = w[::-1].copy()\n",
        "    return w.astype(np.float32)\n",
        "\n",
        "\n",
        "def build_a1_kernel_torch(cfg: Dict[str, Any], device: torch.device, dtype: torch.dtype) -> torch.Tensor:\n",
        "    return torch.tensor(build_a1_kernel_np(cfg), device=device, dtype=dtype)\n",
        "\n",
        "\n",
        "def init_hist_torch(cfg: Dict[str, Any], batch_size: int, device: torch.device, dtype: torch.dtype) -> torch.Tensor:\n",
        "    H = int(cfg[\"H_delay\"])\n",
        "    x_hist = torch.full((batch_size, H), float(cfg[\"x_hist_const\"]), device=device, dtype=dtype)\n",
        "    x_hist[:, 0] = float(cfg[\"x0\"])\n",
        "    return x_hist\n",
        "\n",
        "\n",
        "def init_hist_np(cfg: Dict[str, Any]) -> np.ndarray:\n",
        "    H = int(cfg[\"H_delay\"])\n",
        "    x_hist = np.full(H, float(cfg[\"x_hist_const\"]), dtype=np.float32)\n",
        "    x_hist[0] = float(cfg[\"x0\"])\n",
        "    return x_hist\n",
        "\n",
        "\n",
        "def delay_integral_torch(w: torch.Tensor, x_hist: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.sum(w * x_hist, dim=1)\n",
        "\n",
        "\n",
        "def shift_hist_torch(x_hist: torch.Tensor, x_new: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.cat([x_new.unsqueeze(1), x_hist[:, :-1]], dim=1)\n",
        "\n",
        "\n",
        "def env_step_torch(cfg: Dict[str, Any], w: torch.Tensor, x_hist: torch.Tensor, v: torch.Tensor, dt: float, stochastic: bool = True) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    x_curr = x_hist[:, 0]\n",
        "    integ = delay_integral_torch(w, x_hist)\n",
        "    drift = float(cfg[\"a0\"]) * x_curr + integ + float(cfg[\"b0\"]) * v\n",
        "\n",
        "    if stochastic:\n",
        "        dB = math.sqrt(dt) * torch.randn_like(x_curr)\n",
        "        x_next = x_curr + drift * dt + float(cfg[\"sigma\"]) * dB\n",
        "    else:\n",
        "        x_next = x_curr + drift * dt\n",
        "\n",
        "    x_hist_next = shift_hist_torch(x_hist, x_next)\n",
        "    return x_hist_next, x_next\n",
        "\n",
        "\n",
        "def env_step_np(cfg: Dict[str, Any], w: np.ndarray, x_hist: np.ndarray, v: float, dt: float, stochastic: bool = True) -> Tuple[np.ndarray, float]:\n",
        "    x_curr = float(x_hist[0])\n",
        "    integ = float(np.sum(w * x_hist))\n",
        "    drift = float(cfg[\"a0\"]) * x_curr + integ + float(cfg[\"b0\"]) * v\n",
        "\n",
        "    if stochastic:\n",
        "        x_next = x_curr + drift * dt + float(cfg[\"sigma\"]) * np.random.randn() * math.sqrt(dt)\n",
        "    else:\n",
        "        x_next = x_curr + drift * dt\n",
        "\n",
        "    x_hist = np.roll(x_hist, 1)\n",
        "    x_hist[0] = np.float32(x_next)\n",
        "    return x_hist, float(x_next)\n",
        "\n",
        "\n",
        "def running_cost_np(cfg: Dict[str, Any], v: float, dt: float) -> float:\n",
        "    return float(cfg[\"R\"]) * (v ** 2) * dt\n",
        "\n",
        "\n",
        "def terminal_cost_np(cfg: Dict[str, Any], x_T: float) -> float:\n",
        "    return float(cfg[\"S0\"]) * x_T\n",
        "\n",
        "\n",
        "def running_cost_torch(cfg: Dict[str, Any], v: torch.Tensor, dt: float) -> torch.Tensor:\n",
        "    return float(cfg[\"R\"]) * (v ** 2) * dt\n",
        "\n",
        "\n",
        "def terminal_cost_torch(cfg: Dict[str, Any], x_T: torch.Tensor) -> torch.Tensor:\n",
        "    return float(cfg[\"S0\"]) * x_T\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 3. Method 1: LSTM-DPO warm-up\n",
        "# =============================================================================\n",
        "class LSTMPolicy(nn.Module):\n",
        "    def __init__(self, input_size: int = 2, hidden_size: int = 64):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTMCell(input_size, hidden_size)\n",
        "        self.head = nn.Linear(hidden_size, 1)\n",
        "        nn.init.constant_(self.head.bias, 1.0)\n",
        "\n",
        "    def init_hidden(self, batch_size: int, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        h0 = torch.zeros(batch_size, self.hidden_size, device=device)\n",
        "        c0 = torch.zeros(batch_size, self.hidden_size, device=device)\n",
        "        return h0, c0\n",
        "\n",
        "    def forward_step(self, t_norm: torch.Tensor, x_t: torch.Tensor, h: torch.Tensor, c: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        inp = torch.stack([t_norm, x_t], dim=-1)\n",
        "        h_next, c_next = self.lstm(inp, (h, c))\n",
        "        v_raw = self.head(h_next).squeeze(-1)\n",
        "        v = torch.relu(v_raw)\n",
        "        return v, h_next, c_next\n",
        "\n",
        "\n",
        "def simulate_pg_batch(policy: LSTMPolicy, cfg: Dict[str, Any], batch_size: int, detach_policy: bool = False, stochastic: bool = True) -> torch.Tensor:\n",
        "    dt = get_dt(cfg)\n",
        "    N = int(cfg[\"N_steps\"])\n",
        "    w = build_a1_kernel_torch(cfg, device=DEVICE, dtype=torch.float32)\n",
        "\n",
        "    x_hist = init_hist_torch(cfg, batch_size, device=DEVICE, dtype=torch.float32)\n",
        "    h, c = policy.init_hidden(batch_size, DEVICE)\n",
        "    cost = torch.zeros(batch_size, device=DEVICE, dtype=torch.float32)\n",
        "\n",
        "    for n in range(N):\n",
        "        t_norm = torch.full((batch_size,), (n * dt) / float(cfg[\"T\"]), device=DEVICE, dtype=torch.float32)\n",
        "        x_curr = x_hist[:, 0]\n",
        "\n",
        "        if detach_policy:\n",
        "            with torch.no_grad():\n",
        "                v, h, c = policy.forward_step(t_norm, x_curr, h, c)\n",
        "        else:\n",
        "            v, h, c = policy.forward_step(t_norm, x_curr, h, c)\n",
        "\n",
        "        cost = cost + running_cost_torch(cfg, v, dt)\n",
        "        x_hist, _ = env_step_torch(cfg, w, x_hist, v, dt, stochastic=stochastic)\n",
        "\n",
        "    x_T = x_hist[:, 0]\n",
        "    cost = cost + terminal_cost_torch(cfg, x_T)\n",
        "    return cost.mean()\n",
        "\n",
        "\n",
        "def warmup_train_pg(policy: LSTMPolicy, cfg: Dict[str, Any]) -> LSTMPolicy:\n",
        "    print(\"\\n[Method 1] Starting PG Warm-up for LSTM-DPO...\")\n",
        "    policy.to(DEVICE).train()\n",
        "    opt = optim.Adam(policy.parameters(), lr=float(cfg[\"lr_pg\"]))\n",
        "\n",
        "    for it in range(int(cfg[\"warmup_iters\"])):\n",
        "        opt.zero_grad()\n",
        "        J = simulate_pg_batch(policy, cfg, batch_size=int(cfg[\"warmup_batch_size\"]), detach_policy=False, stochastic=True)\n",
        "        J.backward()\n",
        "        opt.step()\n",
        "\n",
        "        if (it + 1) % 500 == 0:\n",
        "            print(f\"  Iter {it+1}, Cost J = {J.item():.4f}\")\n",
        "\n",
        "    return policy\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def rollout_pg_deterministic(policy: LSTMPolicy, cfg: Dict[str, Any]) -> Dict[str, List[torch.Tensor]]:\n",
        "    dt = get_dt(cfg)\n",
        "    N = int(cfg[\"N_steps\"])\n",
        "    w = build_a1_kernel_torch(cfg, device=DEVICE, dtype=torch.float32)\n",
        "\n",
        "    batch_size = 1\n",
        "    x_hist = init_hist_torch(cfg, batch_size, device=DEVICE, dtype=torch.float32)\n",
        "    h, c = policy.init_hidden(batch_size, DEVICE)\n",
        "\n",
        "    xs, vs, hs, cs, x_hists = [], [], [], [], []\n",
        "\n",
        "    for n in range(N):\n",
        "        t_norm = torch.full((batch_size,), (n * dt) / float(cfg[\"T\"]), device=DEVICE, dtype=torch.float32)\n",
        "        x_curr = x_hist[:, 0]\n",
        "        v, h, c = policy.forward_step(t_norm, x_curr, h, c)\n",
        "\n",
        "        xs.append(x_curr.clone())\n",
        "        vs.append(v.clone())\n",
        "        hs.append(h.clone())\n",
        "        cs.append(c.clone())\n",
        "        x_hists.append(x_hist.clone())\n",
        "\n",
        "        x_hist, _ = env_step_torch(cfg, w, x_hist, v, dt, stochastic=False)\n",
        "\n",
        "    return {\"xs\": xs, \"vs\": vs, \"hs\": hs, \"cs\": cs, \"x_hists\": x_hists}\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 4. PGDPO projection\n",
        "# =============================================================================\n",
        "def estimate_costate_from_step(policy: LSTMPolicy, cfg: Dict[str, Any], n0: int, x_hist0: torch.Tensor, h0: torch.Tensor, c0: torch.Tensor) -> torch.Tensor:\n",
        "    dt = get_dt(cfg)\n",
        "    N = int(cfg[\"N_steps\"])\n",
        "    M = int(cfg[\"N_mc_stage2\"])\n",
        "    w = build_a1_kernel_torch(cfg, device=DEVICE, dtype=torch.float32)\n",
        "\n",
        "    x0_var = x_hist0[0, 0].detach().clone().requires_grad_(True)\n",
        "    x_curr_batch = x0_var.expand(M)\n",
        "\n",
        "    x_hist_rest = x_hist0[:, 1:].repeat(M, 1)\n",
        "    x_hist = torch.cat([x_curr_batch.unsqueeze(1), x_hist_rest], dim=1)\n",
        "\n",
        "    h = h0.repeat(M, 1)\n",
        "    c = c0.repeat(M, 1)\n",
        "\n",
        "    cost = torch.zeros(M, device=DEVICE, dtype=torch.float32)\n",
        "\n",
        "    for n in range(n0, N):\n",
        "        t_norm = torch.full((M,), (n * dt) / float(cfg[\"T\"]), device=DEVICE, dtype=torch.float32)\n",
        "        v, h, c = policy.forward_step(t_norm, x_hist[:, 0], h, c)\n",
        "        cost = cost + running_cost_torch(cfg, v, dt)\n",
        "        x_hist, _ = env_step_torch(cfg, w, x_hist, v, dt, stochastic=True)\n",
        "\n",
        "    x_T = x_hist[:, 0]\n",
        "    cost = cost + terminal_cost_torch(cfg, x_T)\n",
        "\n",
        "    J_mean = cost.mean()\n",
        "    (grad_x0,) = torch.autograd.grad(J_mean, x0_var, retain_graph=False, create_graph=False)\n",
        "    return grad_x0.detach()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 5. Analytic PMP benchmark\n",
        "# =============================================================================\n",
        "def analytic_pmp_solution(cfg: Dict[str, Any]) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    dt = get_dt(cfg)\n",
        "    N = int(cfg[\"N_steps\"])\n",
        "    H = int(cfg[\"H_delay\"])\n",
        "\n",
        "    w = build_a1_kernel_np(cfg)\n",
        "\n",
        "    A = np.zeros((H, H), dtype=np.float64)\n",
        "    A[0, 0] = 1.0 + dt * float(cfg[\"a0\"]) + dt * float(w[0])\n",
        "    A[0, 1:] = dt * w[1:]\n",
        "    for i in range(1, H):\n",
        "        A[i, i - 1] = 1.0\n",
        "\n",
        "    Y = np.zeros((N + 1, H), dtype=np.float64)\n",
        "    Y[N, 0] = float(cfg[\"S0\"])\n",
        "\n",
        "    AT = A.T\n",
        "    for n in reversed(range(N)):\n",
        "        Y[n] = AT @ Y[n + 1]\n",
        "\n",
        "    v_star = np.zeros(N + 1, dtype=np.float64)\n",
        "    for n in range(N + 1):\n",
        "        v_un = -(float(cfg[\"b0\"]) / (2.0 * float(cfg[\"R\"]))) * Y[n, 0]\n",
        "        v_star[n] = max(0.0, v_un)\n",
        "\n",
        "    t_grid = np.linspace(0.0, float(cfg[\"T\"]), N + 1)\n",
        "    return t_grid, v_star\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 6. Method 2: PPO\n",
        "# =============================================================================\n",
        "\n",
        "class PPOActorCritic(nn.Module):\n",
        "    \"\"\"PPO network for distributed state-delay: input is x_hist window (B,H_delay).\"\"\"\n",
        "    def __init__(self, hidden_size: int, init_log_std: float):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
        "        self.actor_mu = nn.Linear(hidden_size, 1)\n",
        "        self.critic = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        # Gaussian std in raw-action space (scalar, shared)\n",
        "        self.log_std = nn.Parameter(torch.tensor([init_log_std], dtype=torch.float32))\n",
        "\n",
        "        nn.init.constant_(self.actor_mu.bias, 0.5)\n",
        "\n",
        "    def _features(self, x_hist: torch.Tensor) -> torch.Tensor:\n",
        "        # x_hist: (B,H) -> (B,H,1)\n",
        "        x = x_hist.unsqueeze(-1)\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        feat = F.relu(self.fc(h_n[-1]))\n",
        "        return feat\n",
        "\n",
        "    def dist_and_value(self, x_hist: torch.Tensor):\n",
        "        feat = self._features(x_hist)\n",
        "        mu = self.actor_mu(feat)                 # (B,1)\n",
        "        val = self.critic(feat).squeeze(-1)      # (B,)\n",
        "        std = torch.exp(self.log_std).expand_as(mu)\n",
        "        dist = Normal(mu, std)\n",
        "        return dist, val\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _ppo_rollout(policy: PPOActorCritic, cfg: Dict[str, Any], w: torch.Tensor, batch_envs: int):\n",
        "    dt = get_dt(cfg)\n",
        "    T_steps = int(cfg[\"N_steps\"])\n",
        "    H = int(cfg[\"H_delay\"])\n",
        "\n",
        "    # init envs\n",
        "    x_hist = init_hist_torch(cfg, batch_envs, device=DEVICE, dtype=torch.float32)\n",
        "\n",
        "    X_list, A_list, logP_list, V_list, R_list = [], [], [], [], []\n",
        "\n",
        "    for t in range(T_steps):\n",
        "        X_list.append(x_hist.clone())\n",
        "\n",
        "        dist, v = policy.dist_and_value(x_hist)     # v: (B,)\n",
        "        raw = dist.sample()                         # (B,1)\n",
        "        logp = dist.log_prob(raw).squeeze(-1)       # (B,)\n",
        "\n",
        "        v_exec = torch.relu(raw.squeeze(-1))        # executed control (B,)\n",
        "\n",
        "        # step env (stochastic)\n",
        "        x_hist, x_next = env_step_torch(cfg, w, x_hist, v_exec, dt, stochastic=True)\n",
        "\n",
        "        # reward (same sign convention as your old PPO code)\n",
        "        # step reward = - running_cost; terminal add -terminal_cost at last step\n",
        "        r = -float(cfg[\"R\"]) * (v_exec ** 2) * dt\n",
        "        if t == T_steps - 1:\n",
        "            r = r + (-terminal_cost_torch(cfg, x_next))\n",
        "\n",
        "        A_list.append(raw.clone())          # (B,1)\n",
        "        logP_list.append(logp.clone())      # (B,)\n",
        "        V_list.append(v.clone())            # (B,)\n",
        "        R_list.append(r.clone())            # (B,)\n",
        "\n",
        "    X = torch.stack(X_list, dim=0)          # (T,B,H)\n",
        "    A = torch.stack(A_list, dim=0)          # (T,B,1)\n",
        "    logP = torch.stack(logP_list, dim=0)    # (T,B)\n",
        "    V = torch.stack(V_list, dim=0)          # (T,B)\n",
        "    R = torch.stack(R_list, dim=0)          # (T,B)\n",
        "    return X, A, logP, V, R\n",
        "\n",
        "\n",
        "def _ppo_gae(cfg: Dict[str, Any], rewards: torch.Tensor, values: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    gamma = float(cfg[\"ppo_gamma\"])\n",
        "    lam = float(cfg[\"ppo_lambda\"])\n",
        "\n",
        "    T_steps, B = rewards.shape\n",
        "    adv = torch.zeros(T_steps, B, device=rewards.device)\n",
        "    gae = torch.zeros(B, device=rewards.device)\n",
        "\n",
        "    for t in reversed(range(T_steps)):\n",
        "        v_t = values[t]\n",
        "        if t == T_steps - 1:\n",
        "            v_next = torch.zeros_like(v_t)\n",
        "            nonterm = 0.0\n",
        "        else:\n",
        "            v_next = values[t + 1]\n",
        "            nonterm = 1.0\n",
        "\n",
        "        delta = rewards[t] + gamma * v_next * nonterm - v_t\n",
        "        gae = delta + gamma * lam * nonterm * gae\n",
        "        adv[t] = gae\n",
        "\n",
        "    ret = adv + values\n",
        "    return ret, adv\n",
        "\n",
        "\n",
        "def train_ppo_standard(cfg: Dict[str, Any], device) -> PPOActorCritic:\n",
        "    print(\"\\n[Method 2] Starting STANDARD PPO Training...\")\n",
        "\n",
        "    # keep your call signature: device can be 'cuda:5' string\n",
        "    dev = torch.device(device)\n",
        "\n",
        "    policy = PPOActorCritic(\n",
        "        hidden_size=int(cfg[\"ppo_hidden\"]),\n",
        "        init_log_std=float(cfg[\"ppo_init_log_std\"]),\n",
        "    ).to(dev)\n",
        "\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=float(cfg[\"ppo_lr\"]))\n",
        "\n",
        "    # kernel on device\n",
        "    w = build_a1_kernel_torch(cfg, device=dev, dtype=torch.float32)\n",
        "\n",
        "    # training budget: total episodes across all envs\n",
        "    total_episodes = int(cfg[\"ppo_total_episodes\"])\n",
        "    envs = int(cfg[\"ppo_num_envs\"])\n",
        "\n",
        "    epochs = int(cfg[\"ppo_epochs\"])\n",
        "    minibatch_size = int(cfg[\"ppo_minibatch_size\"])\n",
        "\n",
        "    clip_eps = float(cfg[\"ppo_clip_eps\"])\n",
        "    ent_coef = float(cfg[\"ppo_entropy_coef\"])\n",
        "    vf_coef = float(cfg[\"ppo_value_coef\"])\n",
        "    max_gn = float(cfg[\"ppo_max_grad_norm\"])\n",
        "\n",
        "    episodes_done = 0\n",
        "    update_idx = 0\n",
        "\n",
        "    while episodes_done < total_episodes:\n",
        "        update_idx += 1\n",
        "        batch_envs = min(envs, total_episodes - episodes_done)\n",
        "        episodes_done += batch_envs\n",
        "\n",
        "        policy.eval()\n",
        "        with torch.no_grad():\n",
        "            X, A, logP_old, V, R = _ppo_rollout(policy, cfg, w, batch_envs=batch_envs)\n",
        "\n",
        "        # GAE + returns\n",
        "        returns, adv = _ppo_gae(cfg, R, V)\n",
        "\n",
        "        T_steps, B, H = X.shape\n",
        "        Xf = X.reshape(T_steps * B, H)\n",
        "        Af = A.reshape(T_steps * B, 1)\n",
        "        logP_old_f = logP_old.reshape(T_steps * B).detach()\n",
        "        returns_f = returns.reshape(T_steps * B).detach()\n",
        "        adv_f = adv.reshape(T_steps * B).detach()\n",
        "\n",
        "        # advantage normalization\n",
        "        adv_f = (adv_f - adv_f.mean()) / (adv_f.std() + 1e-8)\n",
        "\n",
        "        dataset_size = Xf.size(0)\n",
        "        mb = min(minibatch_size, dataset_size)\n",
        "\n",
        "        policy.train()\n",
        "        for _ in range(epochs):\n",
        "            perm = torch.randperm(dataset_size, device=dev)\n",
        "            for start in range(0, dataset_size, mb):\n",
        "                idx = perm[start:start + mb]\n",
        "\n",
        "                dist, v_new = policy.dist_and_value(Xf[idx])\n",
        "                logP_new = dist.log_prob(Af[idx]).squeeze(-1)     # (mb,)\n",
        "                entropy = dist.entropy().squeeze(-1).mean()\n",
        "\n",
        "                ratio = torch.exp(logP_new - logP_old_f[idx])\n",
        "                surr1 = ratio * adv_f[idx]\n",
        "                surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * adv_f[idx]\n",
        "                actor_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "                critic_loss = (v_new - returns_f[idx]).pow(2).mean()\n",
        "\n",
        "                loss = actor_loss + vf_coef * critic_loss - ent_coef * entropy\n",
        "\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(policy.parameters(), max_gn)\n",
        "                optimizer.step()\n",
        "\n",
        "        if update_idx % 200 == 0:\n",
        "            avg_obj = float((-R).sum(dim=0).mean().item())  # rough diagnostic (sum of costs)\n",
        "            print(f\"  [PPO] update {update_idx} | episodes_done={episodes_done}/{total_episodes} | avg_cost_sumâ‰ˆ{avg_obj:.4f}\")\n",
        "\n",
        "    return policy\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_ppo_deterministic(policy: PPOActorCritic, cfg: Dict[str, Any]) -> np.ndarray:\n",
        "    policy.eval()\n",
        "    dt = get_dt(cfg)\n",
        "    N = int(cfg[\"N_steps\"])\n",
        "    w = build_a1_kernel_torch(cfg, device=DEVICE, dtype=torch.float32)\n",
        "\n",
        "    x_hist = init_hist_torch(cfg, batch_size=1, device=DEVICE, dtype=torch.float32)\n",
        "    v_traj: List[float] = []\n",
        "\n",
        "    for t in range(N + 1):\n",
        "        dist, _ = policy.dist_and_value(x_hist)      # dist.mean: (1,1)\n",
        "        mu = dist.mean.squeeze(-1)                   # (1,)\n",
        "        v = torch.relu(mu)                           # (1,)\n",
        "        v_traj.append(float(v.item()))\n",
        "\n",
        "        if t < N:\n",
        "            x_hist, _ = env_step_torch(cfg, w, x_hist, v, dt, stochastic=False)\n",
        "\n",
        "    return np.array(v_traj, dtype=np.float64)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 7. Metrics / Plot\n",
        "# =============================================================================\n",
        "def compute_metrics(gt: np.ndarray, pred: np.ndarray) -> Tuple[float, float]:\n",
        "    m = min(len(gt), len(pred))\n",
        "    rmse = float(np.sqrt(np.mean((pred[:m] - gt[:m]) ** 2)))\n",
        "    mae = float(np.mean(np.abs(pred[:m] - gt[:m])))\n",
        "    return rmse, mae\n",
        "\n",
        "\n",
        "def print_metrics_table(gt: np.ndarray, preds: Dict[str, np.ndarray]) -> None:\n",
        "    print(f\"\\n{'Algorithm':<15} | {'RMSE':<10} | {'MAE':<10}\")\n",
        "    print(\"-\" * 42)\n",
        "    for name, arr in preds.items():\n",
        "        rmse, mae = compute_metrics(gt, arr)\n",
        "        print(f\"{name:<15} | {rmse:.5f}    | {mae:.5f}\")\n",
        "\n",
        "\n",
        "def plot_controls(t_grid: np.ndarray,\n",
        "                  v_star: np.ndarray,\n",
        "                  v_lstm_dpo: np.ndarray,\n",
        "                  v_ppo: np.ndarray,\n",
        "                  v_pgdpo: np.ndarray,\n",
        "                  ppo: bool) -> None:\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    plt.plot(t_grid, v_star, \"k-\", lw=2, label=\"Benchmark\")\n",
        "    plt.plot(t_grid, v_lstm_dpo, \"b:\", label=\"LSTM-DPO\")\n",
        "    if ppo:\n",
        "        plt.plot(t_grid, v_ppo, \"m-\", label=\"PPO\")\n",
        "    plt.plot(t_grid, v_pgdpo, \"r--\", label=\"PGDPO\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Advertising Expenditure\")\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.1)\n",
        "    plt.tight_layout()\n",
        "    #if ppo:\n",
        "    #    plt.savefig(\"Benchmark3_control_ppo.pdf\", bbox_inches=\"tight\")\n",
        "    #else:\n",
        "    #    plt.savefig(\"Benchmark3_control.pdf\", bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 8. Final comparison\n",
        "# =============================================================================\n",
        "def run_full_comparison(policy_pg: LSTMPolicy, ppo_policy: PPOActorCritic, cfg: Dict[str, Any]) -> None:\n",
        "    print(\"\\n=== Running Final Comparison ===\")\n",
        "\n",
        "    t_grid, v_star = analytic_pmp_solution(cfg)\n",
        "\n",
        "    # LSTM-DPO deterministic rollout\n",
        "    policy_pg.eval()\n",
        "    rollout = rollout_pg_deterministic(policy_pg, cfg)\n",
        "\n",
        "    v_pg_raw: List[float] = []\n",
        "    v_pgdpo: List[float] = []\n",
        "\n",
        "    print(\"  Calculating PGDPO projections...\")\n",
        "    N = int(cfg[\"N_steps\"])\n",
        "\n",
        "    for n in range(N):\n",
        "        if n == 0:\n",
        "            t_start = time.time()\n",
        "\n",
        "        v_net = float(rollout[\"vs\"][n].item())\n",
        "\n",
        "        lam = estimate_costate_from_step(\n",
        "            policy_pg, cfg, n, rollout[\"x_hists\"][n], rollout[\"hs\"][n], rollout[\"cs\"][n]\n",
        "        )\n",
        "\n",
        "        v_proj = max(0.0, -(float(cfg[\"b0\"]) / (2.0 * float(cfg[\"R\"]))) * float(lam.item()))\n",
        "\n",
        "        if n == 0:\n",
        "            t_end = time.time()\n",
        "            print(f\"--> [TIME CHECK] 1 Step Projection Time: {t_end - t_start:.6f} seconds\")\n",
        "\n",
        "        v_pg_raw.append(v_net)\n",
        "        v_pgdpo.append(v_proj)\n",
        "\n",
        "    v_pg_raw.append(v_pg_raw[-1])\n",
        "    v_pgdpo.append(v_pgdpo[-1])\n",
        "\n",
        "    # PPO deterministic evaluation\n",
        "    print(\"  Calculating PPO trajectory (deterministic)...\")\n",
        "    v_ppo = eval_ppo_deterministic(ppo_policy, cfg)\n",
        "\n",
        "    # align lengths\n",
        "    v_pg_raw_np = np.array(v_pg_raw, dtype=np.float64)\n",
        "    v_pgdpo_np = np.array(v_pgdpo, dtype=np.float64)\n",
        "\n",
        "    m = min(len(v_star), len(v_pg_raw_np), len(v_pgdpo_np), len(v_ppo), len(t_grid))\n",
        "    v_star_m = v_star[:m]\n",
        "    v_lstm_m = v_pg_raw_np[:m]\n",
        "    v_pgdpo_m = v_pgdpo_np[:m]\n",
        "    v_ppo_m = v_ppo[:m]\n",
        "    t_m = t_grid[:m]\n",
        "\n",
        "    preds = {\n",
        "        \"LSTM-DPO\": v_lstm_m,\n",
        "        \"PPO\": v_ppo_m,\n",
        "        \"PGDPO\": v_pgdpo_m,\n",
        "    }\n",
        "    print_metrics_table(v_star_m, preds)\n",
        "\n",
        "    plot_controls(t_m, v_star_m, v_lstm_m, v_ppo_m, v_pgdpo_m,ppo=True)\n",
        "    plot_controls(t_m, v_star_m, v_lstm_m, v_ppo_m, v_pgdpo_m,ppo=False)\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    set_seed(int(CFG[\"seed\"]))\n",
        "\n",
        "    # 1) LSTM-DPO warm-up (unchanged)\n",
        "    policy_pg = LSTMPolicy(input_size=2, hidden_size=64)\n",
        "    policy_pg = warmup_train_pg(policy_pg, CFG)\n",
        "\n",
        "    # 2) Standard PPO training (new)\n",
        "    ppo_policy = train_ppo_standard(CFG, CFG[\"device\"])\n",
        "\n",
        "    # 3) Final comparison\n",
        "    run_full_comparison(policy_pg, ppo_policy, CFG)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "8OdgM-t68WDv"
      },
      "id": "8OdgM-t68WDv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-PQmB6b99c6"
      },
      "id": "x-PQmB6b99c6",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}